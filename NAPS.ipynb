{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc44269",
   "metadata": {},
   "source": [
    "# Pyds Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd1387d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A framework for performing computations in the Dempster-Shafer theory.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "from itertools import chain, combinations\n",
    "from functools import partial, reduce\n",
    "from operator import mul\n",
    "from math import log, fsum, sqrt\n",
    "from random import random, shuffle, uniform\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import numpy\n",
    "    try:\n",
    "        from scipy.stats import chi2\n",
    "        from scipy.optimize import fmin_cobyla\n",
    "    except:\n",
    "        print('SciPy not found: some features will not work.', file=sys.stderr)\n",
    "except:\n",
    "    print('NumPy not found: some features will not work.', file=sys.stderr)\n",
    "\n",
    "\n",
    "class MassFunction(dict):\n",
    "    \"\"\"\n",
    "    A Dempster-Shafer mass function (basic probability assignment) based on a dictionary.\n",
    "    \n",
    "    Both normalized and unnormalized mass functions are supported.\n",
    "    The underlying frame of discernment is assumed to be discrete.\n",
    "    \n",
    "    Hypotheses and their associated mass values can be added/changed/removed using the standard dictionary methods.\n",
    "    Each hypothesis can be an arbitrary sequence which is automatically converted to a 'frozenset', meaning its elements must be hashable.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, source=None):\n",
    "        \"\"\"\n",
    "        Creates a new mass function.\n",
    "        \n",
    "        If 'source' is not None, it is used to initialize the mass function.\n",
    "        It can either be a dictionary mapping hypotheses to non-negative mass values\n",
    "        or an iterable containing tuples consisting of a hypothesis and a corresponding mass value.\n",
    "        \"\"\"\n",
    "        if source != None:\n",
    "            if isinstance(source, dict):\n",
    "                source = source.items()\n",
    "            for (h, v) in source:\n",
    "                self[h] += v\n",
    "    \n",
    "    @staticmethod\n",
    "    def _convert(hypothesis):\n",
    "        \"\"\"Convert hypothesis to a 'frozenset' in order to make it hashable.\"\"\"\n",
    "        if isinstance(hypothesis, frozenset):\n",
    "            return hypothesis\n",
    "        else:\n",
    "            return frozenset(hypothesis)\n",
    "    \n",
    "    @staticmethod\n",
    "    def gbt(likelihoods, normalization=True, sample_count=None):\n",
    "        \"\"\"\n",
    "        Constructs a mass function using the generalized Bayesian theorem.\n",
    "        For more information, see Smets. 1993. Belief functions: \n",
    "        The disjunctive rule of combination and the generalized Bayesian theorem. International Journal of Approximate Reasoning. \n",
    "        \n",
    "        'likelihoods' specifies the conditional plausibilities for a set of singleton hypotheses.\n",
    "        It can either be a dictionary mapping singleton hypotheses to plausibilities or an iterable\n",
    "        containing tuples consisting of a singleton hypothesis and a corresponding plausibility value.\n",
    "        \n",
    "        'normalization' determines whether the resulting mass function is normalized, i.e., whether m({}) == 0.\n",
    "        \n",
    "        If 'sample_count' is not None, the true mass function is approximated using the specified number of samples.\n",
    "        \"\"\"\n",
    "        m = MassFunction()\n",
    "        if isinstance(likelihoods, dict):\n",
    "            likelihoods = list(likelihoods.items())\n",
    "        # filter trivial likelihoods 0 and 1\n",
    "        ones = [h for (h, l) in likelihoods if l >= 1.0]\n",
    "        likelihoods = [(h, l) for (h, l) in likelihoods if 0.0 < l < 1.0]\n",
    "        if sample_count == None:   # deterministic\n",
    "            def traverse(m, likelihoods, ones, index, hyp, mass):\n",
    "                if index == len(likelihoods):\n",
    "                    m[hyp + ones] = mass\n",
    "                else:\n",
    "                    traverse(m, likelihoods, ones, index + 1, hyp + [likelihoods[index][0]], mass * likelihoods[index][1])\n",
    "                    traverse(m, likelihoods, ones, index + 1, hyp, mass * (1.0 - likelihoods[index][1]))\n",
    "            traverse(m, likelihoods, ones, 0, [], 1.0)\n",
    "            if normalization:\n",
    "                m.normalize()\n",
    "        else:   # Monte-Carlo\n",
    "            if normalization:\n",
    "                empty_mass = reduce(mul, [1.0 - l[1] for l in likelihoods], 1.0)\n",
    "            for _ in range(sample_count):\n",
    "                rv = [random() for _ in range(len(likelihoods))]\n",
    "                subtree_mass = 1.0\n",
    "                hyp = set(ones)\n",
    "                for k in range(len(likelihoods)):\n",
    "                    l = likelihoods[k][1]\n",
    "                    p_t = l * subtree_mass\n",
    "                    p_f = (1.0 - l) * subtree_mass\n",
    "                    if normalization and not hyp: # avoid empty hypotheses in the normalized case\n",
    "                        p_f -= empty_mass\n",
    "                    if p_t > rv[k] * (p_t + p_f):\n",
    "                        hyp.add(likelihoods[k][0])\n",
    "                    else:\n",
    "                        subtree_mass *= 1 - l # only relevant for the normalized empty case\n",
    "                m[hyp] += 1.0 / sample_count\n",
    "        return m\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_bel(bel):\n",
    "        \"\"\"\n",
    "        Creates a mass function from a corresponding belief function.\n",
    "        \n",
    "        'bel' is a dictionary mapping hypotheses to belief values (like the dictionary returned by 'bel(None)').\n",
    "        \"\"\"\n",
    "        m = MassFunction()\n",
    "        for h1 in bel.keys():\n",
    "            v = fsum([bel[h2] * (-1)**(len(h1 - h2)) for h2 in powerset(h1)])\n",
    "            if v > 0:\n",
    "                m[h1] = v\n",
    "        mass_sum = fsum(m.values())\n",
    "        if mass_sum < 1.0:\n",
    "            m[frozenset()] = 1.0 - mass_sum\n",
    "        return m\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_pl(pl):\n",
    "        \"\"\"\n",
    "        Creates a mass function from a corresponding plausibility function.\n",
    "        \n",
    "        'pl' is a dictionary mapping hypotheses to plausibility values (like the dictionary returned by 'pl(None)').\n",
    "        \"\"\"\n",
    "        frame = max(pl.keys(), key=len)\n",
    "        bel_theta = pl[frame]\n",
    "        bel = {frozenset(frame - h):bel_theta - v for (h, v) in pl.items()} # follows from bel(-A) = bel(frame) - pl(A)\n",
    "        return MassFunction.from_bel(bel)\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_q(q):\n",
    "        \"\"\"\n",
    "        Creates a mass function from a corresponding commonality function.\n",
    "        \n",
    "        'q' is a dictionary mapping hypotheses to commonality values (like the dictionary returned by 'q(None)').\n",
    "        \"\"\"\n",
    "        m = MassFunction()\n",
    "        frame = max(q.keys(), key=len)\n",
    "        for h1 in q.keys():\n",
    "            v = fsum([q[h1 | h2] * (-1)**(len(h2 - h1)) for h2 in powerset(frame - h1)])\n",
    "            if v > 0:\n",
    "                m[h1] = v\n",
    "        mass_sum = fsum(m.values())\n",
    "        if mass_sum < 1.0:\n",
    "            m[frozenset()] = 1.0 - mass_sum\n",
    "        return m\n",
    "    \n",
    "    def __missing__(self, key):\n",
    "        \"\"\"Return 0 mass for hypotheses that are not contained.\"\"\"\n",
    "        return 0.0\n",
    "    \n",
    "    def __copy__(self):\n",
    "        c = MassFunction()\n",
    "        for k, v in self.items():\n",
    "            c[k] = v\n",
    "        return c\n",
    "    \n",
    "    def copy(self):\n",
    "        \"\"\"Creates a shallow copy of the mass function.\"\"\"\n",
    "        return self.__copy__()\n",
    "    \n",
    "    def __contains__(self, hypothesis):\n",
    "        return dict.__contains__(self, MassFunction._convert(hypothesis))\n",
    "    \n",
    "    def __getitem__(self, hypothesis):\n",
    "        return dict.__getitem__(self, MassFunction._convert(hypothesis))\n",
    "    \n",
    "    def __setitem__(self, hypothesis, value):\n",
    "        \"\"\"\n",
    "        Adds or updates the mass value of a hypothesis.\n",
    "        \n",
    "        'hypothesis' is automatically converted to a 'frozenset' meaning its elements must be hashable.\n",
    "        In case of a negative mass value, a ValueError is raised.\n",
    "        \"\"\"\n",
    "        if value < 0.0:\n",
    "            raise ValueError(\"mass value is negative: %f\" % value)\n",
    "        dict.__setitem__(self, MassFunction._convert(hypothesis), value)\n",
    "    \n",
    "    def __delitem__(self, hypothesis):\n",
    "        return dict.__delitem__(self, MassFunction._convert(hypothesis))\n",
    "    \n",
    "    def frame(self):\n",
    "        \"\"\"\n",
    "        Returns the frame of discernment of the mass function as a 'frozenset'.\n",
    "        \n",
    "        The frame of discernment is the union of all contained hypotheses.\n",
    "        In case the mass function does not contain any hypotheses, an empty set is returned.\n",
    "        \"\"\"\n",
    "        if not self:\n",
    "            return frozenset()\n",
    "        else:\n",
    "            return frozenset.union(*self.keys())\n",
    "    \n",
    "    def singletons(self):\n",
    "        \"\"\"\n",
    "        Returns the set of all singleton hypotheses.\n",
    "        \n",
    "        Like 'frame()', except that each singleton is wrapped in a frozenset\n",
    "        and can thus be directly passed to methods like 'bel()'.\n",
    "        \"\"\"\n",
    "        return {frozenset((s,)) for s in self.frame()}\n",
    "    \n",
    "    def focal(self):\n",
    "        \"\"\"\n",
    "        Returns the set of all focal hypotheses.\n",
    "        \n",
    "        A focal hypothesis has a mass value greater than 0.\n",
    "        \"\"\"\n",
    "        return {h for (h, v) in self.items() if v > 0}\n",
    "    \n",
    "    def core(self, *mass_functions):\n",
    "        \"\"\"\n",
    "        Returns the core of one or more mass functions as a 'frozenset'.\n",
    "        \n",
    "        The core of a single mass function is the union of all its focal hypotheses.\n",
    "        In case a mass function does not contain any focal hypotheses, its core is an empty set.\n",
    "        If multiple mass functions are given, their combined core (intersection of all single cores) is returned.\n",
    "        \"\"\"\n",
    "        if mass_functions:\n",
    "            return frozenset.intersection(self.core(), *[m.core() for m in mass_functions])\n",
    "        else:\n",
    "            focal = self.focal()\n",
    "            if not focal:\n",
    "                return frozenset()\n",
    "            else:\n",
    "                return frozenset.union(*focal)\n",
    "    \n",
    "    def all(self):\n",
    "        \"\"\"Returns an iterator over all subsets of the frame of discernment, including the empty set.\"\"\"\n",
    "        return powerset(self.frame())\n",
    "    \n",
    "    def bel(self, hypothesis=None):\n",
    "        \"\"\"\n",
    "        Computes either the belief of 'hypothesis' or the entire belief function (hypothesis=None).\n",
    "        \n",
    "        If 'hypothesis' is None (default), a dictionary mapping hypotheses to their respective belief values is returned.\n",
    "        Otherwise, the belief of 'hypothesis' is returned.\n",
    "        In this case, 'hypothesis' is automatically converted to a 'frozenset' meaning its elements must be hashable.\n",
    "        \"\"\"\n",
    "        if hypothesis is None:\n",
    "            return {h:self.bel(h) for h in powerset(self.core())}\n",
    "        else:\n",
    "            hypothesis = MassFunction._convert(hypothesis)\n",
    "            if not hypothesis:\n",
    "                return 0.0\n",
    "            else:\n",
    "                return fsum([v for (h, v) in self.items() if h and hypothesis.issuperset(h)])\n",
    "    \n",
    "    def pl(self, hypothesis=None):\n",
    "        \"\"\"\n",
    "        Computes either the plausibility of 'hypothesis' or the entire plausibility function (hypothesis=None).\n",
    "        \n",
    "        If 'hypothesis' is None (default), a dictionary mapping hypotheses to their respective plausibility values is returned.\n",
    "        Otherwise, the plausibility of 'hypothesis' is returned.\n",
    "        In this case, 'hypothesis' is automatically converted to a 'frozenset' meaning its elements must be hashable.\n",
    "        \"\"\"\n",
    "        if hypothesis is None:\n",
    "            return {h:self.pl(h) for h in powerset(self.core())}\n",
    "        else:\n",
    "            hypothesis = MassFunction._convert(hypothesis)\n",
    "            if not hypothesis:\n",
    "                return 0.0\n",
    "            else:\n",
    "                return fsum([v for (h, v) in self.items() if hypothesis & h])\n",
    "    \n",
    "    def q(self, hypothesis=None):\n",
    "        \"\"\"\n",
    "        Computes either the commonality of 'hypothesis' or the entire commonality function (hypothesis=None).\n",
    "        \n",
    "        If 'hypothesis' is None (default), a dictionary mapping hypotheses to their respective commonality values is returned.\n",
    "        Otherwise, the commonality of 'hypothesis' is returned.\n",
    "        In this case, 'hypothesis' is automatically converted to a 'frozenset' meaning its elements must be hashable.\n",
    "        \"\"\"\n",
    "        if hypothesis is None:\n",
    "            return {h:self.q(h) for h in powerset(self.core())}\n",
    "        else:\n",
    "            if not hypothesis:\n",
    "                return 1.0\n",
    "            else:\n",
    "                return fsum([v for (h, v) in self.items() if h.issuperset(hypothesis)])\n",
    "    \n",
    "    def __and__(self, mass_function):\n",
    "        \"\"\"Shorthand for 'combine_conjunctive(mass_function)'.\"\"\"\n",
    "        return self.combine_conjunctive(mass_function)\n",
    "    \n",
    "    def __or__(self, mass_function):\n",
    "        \"\"\"Shorthand for 'combine_disjunctive(mass_function)'.\"\"\"\n",
    "        return self.combine_disjunctive(mass_function)\n",
    "    \n",
    "    def __str__(self):\n",
    "        hyp = sorted([(v, h) for (h, v) in self.items()], reverse=True)\n",
    "        return \"{\" + \"; \".join([str(set(h)) + \":\" + str(v) for (v, h) in hyp]) + \"}\"\n",
    "    \n",
    "    def __mul__(self, scalar):\n",
    "        if not isinstance(scalar, float):\n",
    "            raise TypeError('Can only multiply by a float value.')\n",
    "        m = MassFunction()\n",
    "        for (h, v) in self.items():\n",
    "            m[h] = v * scalar\n",
    "        return m\n",
    "    \n",
    "    def __rmul__(self, scalar):\n",
    "        return self.__mul__(scalar)\n",
    "    \n",
    "    def __add__(self, m):\n",
    "        if not isinstance(m, MassFunction):\n",
    "            raise TypeError('Can only add two mass functions.')\n",
    "        result = self.copy()\n",
    "        for (h, v) in m.items():\n",
    "            result[h] += v\n",
    "        return result\n",
    "    \n",
    "    def weight_function(self):\n",
    "        \"\"\"\n",
    "        Computes the weight function corresponding to this mass function.\n",
    "        \"\"\"\n",
    "        weights = dict()\n",
    "        q = self.q()\n",
    "        theta = self.frame()\n",
    "        for h in powerset(theta):\n",
    "            if len(h) < len(theta): # weight is undefined for theta\n",
    "                sets = [h | c for c in powerset(theta - h)]\n",
    "                q_even = reduce(mul, [q[h2] for h2 in sets if len(h2) % 2 == 0], 1.0)\n",
    "                q_odd = reduce(mul, [q[h2] for h2 in sets if len(h2) % 2 == 1], 1.0)\n",
    "                if len(h) % 2 == 0:\n",
    "                    weights[h] = q_odd / q_even\n",
    "                else:\n",
    "                    weights[h] = q_even / q_odd\n",
    "        return weights\n",
    "    \n",
    "    def combine_conjunctive(self, mass_function, normalization=True, sample_count=None, importance_sampling=False):\n",
    "        \"\"\"\n",
    "        Conjunctively combines the mass function with another mass function and returns the combination as a new mass function.\n",
    "        \n",
    "        The other mass function is assumed to be defined over the same frame of discernment.\n",
    "        If 'mass_function' is not of type MassFunction, it is assumed to be an iterable containing multiple mass functions that are iteratively combined.\n",
    "        \n",
    "        If the mass functions are flatly contracting or if one of the mass functions is empty, an empty mass function is returned.\n",
    "        \n",
    "        'normalization' determines whether the resulting mass function is normalized (default is True).\n",
    "         \n",
    "        If 'sample_count' is not None, the true combination is approximated using the specified number of samples.\n",
    "        In this case, 'importance_sampling' determines the method of approximation (only if normalization=True, otherwise 'importance_sampling' is ignored).\n",
    "        The default method (importance_sampling=False) independently generates samples from both mass functions and computes their intersections.\n",
    "        If importance_sampling=True, importance sampling is used to avoid empty intersections, which leads to a lower approximation error but is also slower.\n",
    "        This method should be used if there is significant evidential conflict between the mass functions.\n",
    "        \"\"\"\n",
    "        return self._combine(mass_function, rule=lambda s1, s2: s1 & s2, normalization=normalization, sample_count=sample_count, importance_sampling=importance_sampling)\n",
    "    \n",
    "    def combine_disjunctive(self, mass_function, sample_count=None):\n",
    "        \"\"\"\n",
    "        Disjunctively combines the mass function with another mass function and returns the combination as a new mass function.\n",
    "        \n",
    "        The other mass function is assumed to be defined over the same frame of discernment.\n",
    "        If 'mass_function' is not of type MassFunction, it is assumed to be an iterable containing multiple mass functions that are iteratively combined.\n",
    "        \n",
    "        If 'sample_count' is not None, the true combination is approximated using the specified number of samples.\n",
    "        \"\"\"\n",
    "        return self._combine(mass_function, rule=lambda s1, s2: s1 | s2, normalization=False, sample_count=sample_count, importance_sampling=False)\n",
    "    \n",
    "    def combine_cautious(self, mass_function):\n",
    "        \"\"\"\n",
    "        Combines the mass function with another mass function using the cautious rule and returns the combination as a new mass function.\n",
    "        \n",
    "        For more details, see:\n",
    "        T. Denoeux (2008), \"Conjunctive and disjunctive combination of belief functions induced by nondistinct bodies of evidence\",\n",
    "        Artificial Intelligence 172, 234-264.\n",
    "        \"\"\"\n",
    "        w1 = self.weight_function()\n",
    "        w2 = mass_function.weight_function()\n",
    "        w_min = {h:min(w1[h], w2[h]) for h in w1}\n",
    "        theta = self.frame()\n",
    "        m = MassFunction({theta:1.0})\n",
    "        for h, w in w_min.items():\n",
    "            m_simple = MassFunction({theta:w, h:1.0 - w})\n",
    "            m = m.combine_conjunctive(m_simple, normalization=False)\n",
    "        return m\n",
    "    \n",
    "    def _combine(self, mass_function, rule, normalization, sample_count, importance_sampling):\n",
    "        \"\"\"Helper method for combining two or more mass functions.\"\"\"\n",
    "        combined = self\n",
    "        if isinstance(mass_function, MassFunction):\n",
    "            mass_function = [mass_function] # wrap single mass function\n",
    "        for m in mass_function:\n",
    "            if not isinstance(m, MassFunction):\n",
    "                raise TypeError(\"expected type MassFunction but got %s; make sure to use keyword arguments for anything other than mass functions\" % type(m))\n",
    "            if sample_count == None:\n",
    "                combined = combined._combine_deterministic(m, rule)\n",
    "            else:\n",
    "                if importance_sampling and normalization:\n",
    "                    combined = combined._combine_importance_sampling(m, sample_count)\n",
    "                else:\n",
    "                    combined = combined._combine_direct_sampling(m, rule, sample_count)\n",
    "        if normalization:\n",
    "            return combined.normalize()\n",
    "        else:\n",
    "            return combined\n",
    "    \n",
    "    def _combine_deterministic(self, mass_function, rule):\n",
    "        \"\"\"Helper method for deterministically combining two mass functions.\"\"\"\n",
    "        combined = MassFunction()\n",
    "        for (h1, v1) in self.items():\n",
    "            for (h2, v2) in mass_function.items():\n",
    "                combined[rule(h1, h2)] += v1 * v2\n",
    "        return combined\n",
    "    \n",
    "    def _combine_direct_sampling(self, mass_function, rule, sample_count):\n",
    "        \"\"\"Helper method for approximatively combining two mass functions using direct sampling.\"\"\"\n",
    "        combined = MassFunction()\n",
    "        samples1 = self.sample(sample_count)\n",
    "        samples2 = mass_function.sample(sample_count)\n",
    "        for i in range(sample_count):\n",
    "            combined[rule(samples1[i], samples2[i])] += 1.0 / sample_count\n",
    "        return combined\n",
    "    \n",
    "    def _combine_importance_sampling(self, mass_function, sample_count):\n",
    "        \"\"\"Helper method for approximatively combining two mass functions using importance sampling.\"\"\"\n",
    "        combined = MassFunction()\n",
    "        for (s1, n) in self.sample(sample_count, as_dict=True).items():\n",
    "            weight = mass_function.pl(s1)\n",
    "            for s2 in mass_function.condition(s1).sample(n):\n",
    "                combined[s2] += weight\n",
    "        return combined\n",
    "    \n",
    "    def combine_gbt(self, likelihoods, normalization=True, sample_count=None, importance_sampling=True):\n",
    "        \"\"\"\n",
    "        Conjunctively combines the mass function with a mass function obtained from a sequence of\n",
    "        likelihoods via the generalized Bayesian theorem and returns the combination as a new mass function.\n",
    "        \n",
    "        Equivalent to 'combine_conjunctive(MassFunction.gbt(likelihoods))'.\n",
    "        By ignoring incompatible likelihoods, it is generally faster than the former\n",
    "        method and yields a better Monte-Carlo approximation in case of normalization.\n",
    "        \n",
    "        'likelihoods' specifies the conditional plausibilities for a set of singleton hypotheses.\n",
    "        It can either be a dictionary mapping singleton hypotheses to plausibilities or an iterable\n",
    "        containing tuples consisting of a singleton hypothesis and a corresponding plausibility value.\n",
    "        \n",
    "        All arguments except for 'likelihoods' must be specified as keyword arguments.\n",
    "        'normalization' determines whether the resulting mass function is normalized, i.e., whether m({}) == 0.\n",
    "        If 'sample_count' is not None, the true mass function is approximated using the specified number of samples.\n",
    "        See 'combine_conjunctive' for details on the effect of setting 'importance_sampling'.\n",
    "        \"\"\"\n",
    "        core = self.core() # restrict to generally compatible likelihoods\n",
    "        if isinstance(likelihoods, dict):\n",
    "            likelihoods = list(likelihoods.items())\n",
    "        likelihoods = [l for l in likelihoods if l[1] > 0 and l[0] in core]\n",
    "        if sample_count == None: # deterministic\n",
    "            return self.combine_conjunctive(MassFunction.gbt(likelihoods), normalization=normalization)\n",
    "        else: # Monte-Carlo\n",
    "            if not normalization: # only use importance sampling in case of normalization\n",
    "                importance_sampling = False\n",
    "            combined = MassFunction()\n",
    "            for s, n in self.sample(sample_count, as_dict=True).items():\n",
    "                if importance_sampling:\n",
    "                    compatible_likelihoods = [l for l in likelihoods if l[0] in s]\n",
    "                    weight = 1.0 - reduce(mul, [1.0 - l[1] for l in compatible_likelihoods], 1.0)\n",
    "                else:\n",
    "                    compatible_likelihoods = likelihoods\n",
    "                if not compatible_likelihoods:\n",
    "                    continue\n",
    "                if normalization:\n",
    "                    empty_mass = reduce(mul, [1.0 - l[1] for l in compatible_likelihoods], 1.0)\n",
    "                for _ in range(n):\n",
    "                    rv = [random() for _ in range(len(compatible_likelihoods))]\n",
    "                    subtree_mass = 1.0\n",
    "                    hyp = set()\n",
    "                    for k in range(len(compatible_likelihoods)):\n",
    "                        l = compatible_likelihoods[k][1]\n",
    "                        norm = 1.0 if hyp or not normalization else 1.0 - empty_mass / subtree_mass\n",
    "                        if l / norm > rv[k]:\n",
    "                            hyp.add(compatible_likelihoods[k][0])\n",
    "                        else:\n",
    "                            subtree_mass *= 1.0 - l   # only relevant for negative case\n",
    "                    if importance_sampling:\n",
    "                        combined[hyp] += weight\n",
    "                    else:\n",
    "                        combined[hyp & s] += 1.0\n",
    "            if normalization:\n",
    "                return combined.normalize()\n",
    "            else:\n",
    "                return combined\n",
    "    \n",
    "    def condition(self, hypothesis, normalization=True):\n",
    "        \"\"\"\n",
    "        Conditions the mass function with 'hypothesis'.\n",
    "        \n",
    "        'normalization' determines whether the resulting conjunctive combination is normalized (must be specified as a keyword argument).\n",
    "        \n",
    "        Shorthand for self.combine_conjunctive(MassFunction({hypothesis:1.0}), normalization).\n",
    "        \"\"\"\n",
    "        m = MassFunction({MassFunction._convert(hypothesis):1.0})\n",
    "        return self.combine_conjunctive(m, normalization=normalization)\n",
    "    \n",
    "    def conflict(self, mass_function, sample_count=None):\n",
    "        \"\"\"\n",
    "        Calculates the weight of conflict between two or more mass functions.\n",
    "        \n",
    "        If 'mass_function' is not of type MassFunction, it is assumed to be an iterable containing multiple mass functions.\n",
    "        \n",
    "        The weight of conflict is computed as the (natural) logarithm of the normalization constant in Dempster's rule of combination.\n",
    "        Returns infinity in case the mass functions are flatly contradicting.\n",
    "        \"\"\"\n",
    "        # compute full conjunctive combination (could be more efficient)\n",
    "        m = self.combine_conjunctive(mass_function, normalization=False, sample_count=sample_count)\n",
    "        empty = m[frozenset()]\n",
    "        m_sum = fsum(m.values())\n",
    "        diff = m_sum - empty\n",
    "        if diff == 0.0:\n",
    "            return float('inf')\n",
    "        else:\n",
    "            return -log(diff)\n",
    "    \n",
    "    def normalize(self):\n",
    "        \"\"\"\n",
    "        Normalizes the mass function in-place.\n",
    "        \n",
    "        Sets the mass value of the empty set to 0 and scales all other values such that their sum equals 1.\n",
    "        For convenience, the method returns 'self'.\n",
    "        \"\"\"\n",
    "        if frozenset() in self:\n",
    "            del self[frozenset()]\n",
    "        mass_sum = fsum(self.values())\n",
    "        if mass_sum != 1.0:\n",
    "            for (h, v) in self.items():\n",
    "                self[h] = v / mass_sum\n",
    "        return self\n",
    "    \n",
    "    def prune(self):\n",
    "        \"\"\"\n",
    "        Removes all non-focal (0 mass) hypotheses in-place.\n",
    "        \n",
    "        For convenience, the method returns 'self'.\n",
    "        \"\"\" \n",
    "        remove = [h for (h, v) in self.items() if v == 0.0]\n",
    "        for h in remove:\n",
    "            del self[h]\n",
    "        return self\n",
    "    \n",
    "    def markov(self, transition_model, sample_count=None):\n",
    "        \"\"\"\n",
    "        Computes the mass function induced by a prior belief (self) and a transition model.\n",
    "        \n",
    "        The transition model expresses a joint belief over the frame of this mass function and a new frame.\n",
    "        The belief over the frame of this mass function is implicitly assumed to be vacuous.\n",
    "        The transition model is a function returning the conditional belief over the new frame (as a mass function\n",
    "        if sample_count=None) while taking a singleton hypothesis of the current frame as input.\n",
    "        The disjunctive rule of combination is then used to construct the mass function over the new frame.\n",
    "        \n",
    "        If 'sample_count' is not None, the true mass function is approximated using the specified number of samples.\n",
    "        In this case, 'transition_model' is expected to take a second argument stating how many samples from the corresponding conditional mass function should be returned.\n",
    "        The return value in this case is expected to be an iterable over sampled hypotheses from the new frame.\n",
    "         \n",
    "        This method can be used to implement the prediction step for estimation in a hidden Markov process (hence the name).\n",
    "        Under this interpretation, the transition model expresses the mass distribution over successor states given the current state.\n",
    "        \"\"\"\n",
    "        updated = MassFunction()\n",
    "        if sample_count == None: # deterministic\n",
    "            for k, v in self.items():\n",
    "                predicted = None\n",
    "                for e in k:\n",
    "                    if predicted == None:\n",
    "                        predicted = transition_model(e)\n",
    "                    else:\n",
    "                        predicted |= transition_model(e)\n",
    "                for kp, vp in predicted.items():\n",
    "                    updated[kp] += v * vp\n",
    "        else: # Monte-Carlo\n",
    "            for s, n in self.sample(sample_count, as_dict=True).items():\n",
    "                unions = [[] for _ in range(n)]\n",
    "                for e in s:\n",
    "                    ts = transition_model(e, n)\n",
    "                    for i, t in enumerate(ts):\n",
    "                        unions[i].extend(t)\n",
    "                for u in unions:\n",
    "                    updated[u] += 1.0 / sample_count\n",
    "        return updated\n",
    "    \n",
    "    def map(self, function):\n",
    "        \"\"\"\n",
    "        Maps each hypothesis to a new hypothesis using 'function' and returns the new mass function.\n",
    "        \n",
    "        'function' is a function taking a hypothesis as its only input and returning a new hypothesis\n",
    "        (i.e., a sequence that can be converted to a 'frozenset').\n",
    "        \n",
    "        Here are some example use cases:\n",
    "        \n",
    "        1. Vacuous extension to a multi-dimensional frame of discernment (m is defined over\n",
    "        the frame A while the new mass function is defined over the Cartesian product AxB):\n",
    "            \n",
    "            B = {'x', 'y', 'z'}\n",
    "            m.map(lambda h: itertools.product(h, B))\n",
    "        \n",
    "        2. Projection to a lower dimensional frame (m is defined over AxBxC such that each hypothesis is\n",
    "        a set of tuples where each tuple consists of 3 elements; the new mass function is defined over BxC):\n",
    "        \n",
    "            m.map(lambda h: (t[1:] for t in h))\n",
    "        \"\"\"\n",
    "        m = MassFunction()\n",
    "        for (h, v) in self.items():\n",
    "            m[self._convert(function(h))] += v\n",
    "        return m\n",
    "    \n",
    "    def pignistic(self):\n",
    "        \"\"\"Computes the pignistic transformation and returns it as a new mass function consisting only of singletons.\"\"\"\n",
    "        p = MassFunction()\n",
    "        for (h, v) in self.items():\n",
    "            if v > 0.0:\n",
    "                size = float(len(h))\n",
    "                for s in h:\n",
    "                    p[(s,)] += v / size\n",
    "        return p.normalize()\n",
    "    \n",
    "    def local_conflict(self):\n",
    "        \"\"\"\n",
    "        Computes the local conflict measure.\n",
    "        \n",
    "        For more information, see Pal et al. 1993. Uncertainty measures for evidential reasoning II:\n",
    "        A new measure of total uncertainty. International Journal of Approximate Reasoning.\n",
    "        \n",
    "        Only works for normalized mass functions.\n",
    "        If the mass function is unnormalized, the method returns float('nan')\n",
    "        \n",
    "        In case the mass function is a probability function (containing only singleton hypotheses),\n",
    "        it reduces to the classical entropy measure.\n",
    "        \"\"\"\n",
    "        if self[frozenset()] > 0.0:\n",
    "            return float('nan')\n",
    "        c = 0.0\n",
    "        for (h, v) in self.items():\n",
    "            if v > 0.0:\n",
    "                c += v * log(len(h) / v, 2)\n",
    "        return c\n",
    "    \n",
    "    def hartley_measure(self):\n",
    "        \"\"\"\n",
    "        Computes the Hartley-like measure in order to quantify the amount of imprecision.\n",
    "        \n",
    "        For more information, see:\n",
    "        G. J. Klir (1999), \"Uncertainty and information measures for imprecise probabilities: An overview\",\n",
    "        International Symposium on Imprecise Probabilities and Their Applications.\n",
    "        \"\"\"\n",
    "        return fsum([v * log(len(h), 2) for h, v in self.items()])\n",
    "    \n",
    "    def norm(self, m, p=2):\n",
    "        \"\"\"\n",
    "        Computes the p-norm between two mass functions (default is p=2).\n",
    "        \n",
    "        Both mass functions are treated as vectors of mass values.\n",
    "        \"\"\"\n",
    "        d = fsum([(v - m[h])**p for (h, v) in self.items()])\n",
    "        for (h, v) in m.items():\n",
    "            if h not in self:\n",
    "                d += v**p\n",
    "        return d**(1.0 / p)\n",
    "    \n",
    "    def is_compatible(self, m):\n",
    "        \"\"\"\n",
    "        Checks whether another mass function is compatible with this one.\n",
    "        \n",
    "        Compatibility means that the mass value of each hypothesis in 'm' is less than\n",
    "        or equal to the corresponding plausibility given by this mass function.\n",
    "        \"\"\"\n",
    "        return all([self.pl(h) >= v for (h, v) in m.items()])\n",
    "    \n",
    "    def sample(self, n, quantization=True, as_dict=False):\n",
    "        \"\"\"\n",
    "        Returns n random samples from the mass distribution.\n",
    "        \n",
    "        Hypotheses are drawn with a probability proportional to their mass values (with replacement).\n",
    "         \n",
    "        If 'quantization' is True (default), the method performs a quantization of the mass values.\n",
    "        This means the frequency of a hypothesis h in the sample set is at least int(self[h] * n / t) where t is the sum of all mass values.\n",
    "        The remaining sample slots (if any) are filled up according to the remainders of the fractions computed in the first step.\n",
    "        \n",
    "        The parameter 'as_dict' determines the type of the returned value.\n",
    "        If 'as_dict' is False (default), a list of length n is returned.\n",
    "        Otherwise, the result is a dictionary specifying the number of samples for each hypothesis.\n",
    "        \"\"\"\n",
    "        if not isinstance(n, int):\n",
    "            raise TypeError(\"n must be int\")\n",
    "        samples = {h:0 for h in self} if as_dict else []\n",
    "        mass_sum = fsum(self.values())\n",
    "        if quantization:\n",
    "            remainders = []\n",
    "            remaining_sample_count = n\n",
    "            for (h, v) in self.items():\n",
    "                fraction = n * v / mass_sum\n",
    "                quotient = int(fraction)\n",
    "                if quotient > 0:\n",
    "                    if as_dict:\n",
    "                        samples[h] = quotient\n",
    "                    else:\n",
    "                        samples.extend([h] * quotient)\n",
    "                remainders.append((h, fraction - quotient))\n",
    "                remaining_sample_count -= quotient\n",
    "            remainders.sort(reverse=True, key=lambda hv: hv[1])\n",
    "            for h, _ in remainders[:remaining_sample_count]:\n",
    "                if as_dict:\n",
    "                    samples[h] += 1\n",
    "                else:\n",
    "                    samples.append(h)\n",
    "        else:\n",
    "            rv = [uniform(0.0, mass_sum) for _ in range(n)]\n",
    "            hypotheses = sorted(self.items(), reverse=True, key=lambda hv: hv[1])\n",
    "            for i in range(n):\n",
    "                mass = 0.0\n",
    "                for (h, v) in hypotheses:\n",
    "                    mass += v\n",
    "                    if mass >= rv[i]:\n",
    "                        if as_dict:\n",
    "                            samples[h] += 1\n",
    "                        else:\n",
    "                            samples.append(h)\n",
    "                        break\n",
    "        if not as_dict:\n",
    "            shuffle(samples)\n",
    "        return samples\n",
    "    \n",
    "    def is_probabilistic(self):\n",
    "        \"\"\"\n",
    "        Checks whether the mass function is a probability function.\n",
    "        \n",
    "        Returns True if and only if all hypotheses are singletons (normalization is ignored). \n",
    "        \"\"\"\n",
    "        return all([len(h) == 1 for h in self.keys()])\n",
    "    \n",
    "    def sample_probability_distributions(self, n):\n",
    "        \"\"\"\n",
    "        Randomly generates n compatible probability distributions from the mass function.\n",
    "        \n",
    "        The result is a list of n independently sampled probability distributions expressed as mass functions.\n",
    "        This can be useful for estimating various statistical measures like the minimum or maximum entropy consistent with the mass distribution.\n",
    "        \"\"\"\n",
    "        samples = [MassFunction() for _ in range(n)]\n",
    "        for i in range(n):\n",
    "            for (h, v) in self.items():\n",
    "                if len(h) == 1:\n",
    "                    samples[i][h] += v\n",
    "                else:\n",
    "                    rv = [random() for _ in range(len(h))]\n",
    "                    total = fsum(rv)\n",
    "                    for k, s in enumerate(h):\n",
    "                        samples[i][{s}] += rv[k] * v / total\n",
    "        return samples\n",
    "    \n",
    "    def max_bel(self):\n",
    "        \"\"\"\n",
    "        Returns the singleton with the highest belief.\n",
    "        \n",
    "        In case there are multiple singletons with maximum belief, only one of them is returned.\n",
    "        Returns None, if the mass function does not contain any hypotheses.\n",
    "        \"\"\"\n",
    "        return self._max_singleton(self.bel)\n",
    "    \n",
    "    def max_pl(self):\n",
    "        \"\"\"\n",
    "        Returns the singleton with the highest plausibility.\n",
    "        \n",
    "        In case there are multiple singletons with maximum plausibility, only one of them is returned.\n",
    "        Returns None, if the mass function does not contain any hypotheses.\n",
    "        \"\"\"\n",
    "        return self._max_singleton(self.pl)\n",
    "    \n",
    "    def _max_singleton(self, f):\n",
    "        st = self.singletons()\n",
    "        if st:\n",
    "            value_list = [(f(s), s) for s in st]\n",
    "            shuffle(value_list)\n",
    "            return max(value_list)[1]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert a mass function only consisting of singletons to a dictionary by removing each enclosing frozenset.\"\"\"\n",
    "        if not self.is_probabilistic():\n",
    "            raise Exception('mass function must only contain singletons')\n",
    "        return {tuple(h)[0]:v for h, v in self.items()}\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_dict(d):\n",
    "        \"\"\"Convert a dictionary to a mass function by enclosing each key with a frozenset.\"\"\"\n",
    "        if isinstance(d, MassFunction):\n",
    "            return d\n",
    "        else:\n",
    "            return MassFunction({frozenset((h,)):v for h, v in d.items()})\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_possibility(poss):\n",
    "        \"\"\"\n",
    "        Constructs a consonant mass function from a possibility distribution.\n",
    "                \n",
    "        For more information, see:\n",
    "        D. Dubois, H. Prade (1982), \"On several representations of an uncertainty body of evidence\",\n",
    "        Fuzzy Information and Decision Processes, 167-181.\n",
    "        \"\"\"\n",
    "        if isinstance(poss, MassFunction):\n",
    "            poss = poss.to_dict() # remove enclosing sets\n",
    "        H, P = zip(*sorted(poss.items(), key=lambda e: e[1], reverse=True)) # sort possibility values in descending order\n",
    "        m = MassFunction()\n",
    "        m[H] = P[-1]\n",
    "        for i in range(len(H) - 1):\n",
    "            m[H[:i + 1]] = P[i] - P[i + 1]\n",
    "        return m\n",
    "    \n",
    "    @staticmethod\n",
    "    def pignistic_inverse(p):\n",
    "        \"\"\"\n",
    "        Constructs a consonant mass function from a pignistic probability distribution by applying the inverse pignistic transformation.\n",
    "        \n",
    "        For more information, see:\n",
    "        D. Dubois, H. Prade, P. Smets (2008), \"A definition of subjective possibility\",\n",
    "        International Journal of Approximate Reasoning 48 (2), 352-364.\n",
    "        \"\"\"\n",
    "        p = MassFunction.from_dict(p)\n",
    "        poss = MassFunction({h1:fsum([min(p[h1], p[h2]) for h2 in p.keys()]) for h1 in p.keys()})\n",
    "        return MassFunction.from_possibility(poss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _to_array_index(hypothesis, frame):\n",
    "        \"\"\"Map a hypothesis to an array index given a frame of discernment.\"\"\"\n",
    "        index = 0\n",
    "        for i, s in enumerate(frame):\n",
    "            if s in hypothesis:\n",
    "                index += 2**i\n",
    "        return index\n",
    "    \n",
    "    @staticmethod\n",
    "    def _from_array_index(index, frame):\n",
    "        \"\"\"Map an array index to a hypothesis given a frame of discernment.\"\"\"\n",
    "        hypothesis = set()\n",
    "        for i, s in enumerate(frame):\n",
    "            if 2**i & index:\n",
    "                hypothesis.add(s)\n",
    "        return frozenset(hypothesis)\n",
    "    \n",
    "    def to_array(self, frame):\n",
    "        \"\"\"\n",
    "        Convert the mass function to a NumPy array.\n",
    "        \n",
    "        Hypotheses are mapped to array indices using '_to_array_index'.\n",
    "        The resulting array has 2^n entries where n is the size of the frame of discernment.\n",
    "        \"\"\"\n",
    "        a = numpy.zeros(2**len(frame))\n",
    "        for h, v in self.items():\n",
    "            a[MassFunction._to_array_index(h, frame)] = v\n",
    "        return a\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_array(a, frame):\n",
    "        \"\"\"\n",
    "        Convert a NumPy array to a mass function given a frame of discernment.\n",
    "        \n",
    "        Array indices are mapped to hypotheses using '_from_array_index'.\n",
    "        \"\"\"\n",
    "        m = MassFunction()\n",
    "        for i, v in enumerate(a):\n",
    "            if v > 0.0:\n",
    "                m[MassFunction._from_array_index(i, frame)] = v\n",
    "        return m\n",
    "    \n",
    "    @staticmethod\n",
    "    def _confidence_intervals(histogram, alpha):\n",
    "        \"\"\"Compute Goodman confidence intervals.\"\"\"\n",
    "        p_lower = {}\n",
    "        p_upper = {}\n",
    "        a = chi2.ppf(1. - alpha / len(histogram), 1)\n",
    "        n = float(sum(histogram.values()))\n",
    "        for h, n_h in histogram.items():\n",
    "            delta_h = a * (a + 4. * n_h * (n - n_h) / n)\n",
    "            p_lower[h] = (a + 2. * n_h - sqrt(delta_h)) / (2. * (n + a))\n",
    "            p_upper[h] = (a + 2. * n_h + sqrt(delta_h)) / (2. * (n + a))\n",
    "        return p_lower, p_upper\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_samples(histogram, method='idm', alpha=0.05, s=1.0):\n",
    "        \"\"\"\n",
    "        Generate a mass function from an empirical probability distribution that was obtained from a limited number of samples.\n",
    "        This makes the expected deviation of the empirical distribution from the true distribution explicit.\n",
    "        \n",
    "        'histogram' represents the empirical distribution. It is a dictionary mapping each possible event to the respective\n",
    "        number of observations (represented as integers).\n",
    "        \n",
    "        'method' determines the algorithm used for generating the mass function.\n",
    "        Except for method 'bayesian', all algorithms are based on the idea that the true probabilities lie within confidence intervals\n",
    "        represented by the mass function with confidence level 1 - 'alpha'.\n",
    "        \n",
    "        The following modes are supported:\n",
    "        \n",
    "        'idm': Imprecise Dirichlet model. A small amount of mass (controlled by 's') is assigned to the entire frame.\n",
    "        For more information on 'idm', see:\n",
    "        P. Walley (1996), \"Inferences from multinomial data: learning about a bag of marbles\",\n",
    "        Journal of the Royal Statistical Society. Series B (Methodological), 3-57.\n",
    "        \n",
    "        'maxbel': Maximize the total belief by solving a linear program. (Attention: this becomes very computationally expensive\n",
    "        for larger numbers of events.)\n",
    "        \n",
    "        'maxbel-ordered': Similar to 'maxbel' except that the events are assumed to have a natural order (e.g., intervals), in which case\n",
    "        the mass function can be computed analytically and thus much faster.\n",
    "        \n",
    "        For more information on 'maxbel' and 'maxbel-ordered', see:\n",
    "        T. Denoeux (2006), \"Constructing belief functions from sample data using multinomial confidence regions\",\n",
    "        International Journal of Approximate Reasoning 42, 228-252.\n",
    "        \n",
    "        'mcd': Compute the least committed consonant mass function whose pignistic transformation lies within the confidence interval\n",
    "        induced by 'alpha'. Like 'maxbel', it is based on solving a linear program and quickly becomes computationally expensive.\n",
    "        \n",
    "        'mcd-approximate': An approximation of 'mcd' that can be computed much more efficiently.\n",
    "        \n",
    "        For more information on these two methods, see:\n",
    "        A. Aregui, T. Denoeux (2008), \"Constructing consonant belief functions from sample data using confidence sets of pignistic probabilities\",\n",
    "        International Journal of Approximate Reasoning 49, 575-594.\n",
    "        \n",
    "        'bayesian': Construct a Bayesian mass function based on the relative frequencies. In addition, additive smoothing is applied (controlled by 's'). \n",
    "        \n",
    "        In case the sample number is 0, returns a vacuous mass function (or uniform distribution for 'bayesian').\n",
    "        \n",
    "        (Requires SciPy for computing confidence intervals and solving linear programs.)\n",
    "        \"\"\"\n",
    "        if not isinstance(histogram, dict):\n",
    "            raise TypeError('histogram must be of type dict')\n",
    "        for v in histogram.values():\n",
    "            if not isinstance(v, int):\n",
    "                raise TypeError('all histogram values must be of type int')\n",
    "        if not histogram:\n",
    "            return MassFunction()\n",
    "        if sum(histogram.values()) == 0: # return vacuous/uniform belief if there are no samples\n",
    "            vac = MassFunction({tuple(histogram.keys()):1})\n",
    "            if method == 'bayesian':\n",
    "                return vac.pignistic()\n",
    "            else:\n",
    "                return vac\n",
    "        if method == 'bayesian':\n",
    "            return MassFunction({(h,):v + s for h, v in histogram.items()}).normalize()\n",
    "        elif method == 'idm':\n",
    "            return MassFunction._from_samples_idm(histogram, s)\n",
    "        elif method == 'maxbel':\n",
    "            return MassFunction._from_samples_maxbel(histogram, alpha)\n",
    "        elif method == 'maxbel-ordered':\n",
    "            return MassFunction._from_samples_maxbel(histogram, alpha, ordered=True)\n",
    "        elif method == 'mcd':\n",
    "            return MassFunction._from_samples_mcd(histogram, alpha)\n",
    "        elif method == 'mcd-approximate':\n",
    "            return MassFunction._from_samples_mcd(histogram, alpha, approximate=True)\n",
    "        raise ValueError('unknown method: %s' % method)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _from_samples_idm(histogram, s):\n",
    "        \"\"\"\n",
    "        Reference:\n",
    "        P. Walley (1996), \"Inferences from multinomial data: learning about a bag of marbles\",\n",
    "        Journal of the Royal Statistical Society. Series B (Methodological), 3-57.\n",
    "        \"\"\"\n",
    "        total = sum(histogram.values())\n",
    "        m = MassFunction()\n",
    "        for h, c in histogram.items():\n",
    "            m[(h,)] = float(c) / (total + s)\n",
    "        m[MassFunction._convert(histogram.keys())] = float(s) / (total + s)\n",
    "        return m\n",
    "    \n",
    "    @staticmethod\n",
    "    def _from_samples_maxbel(histogram, alpha, ordered=False):\n",
    "        \"\"\"\n",
    "        Reference:\n",
    "        T. Denoeux (2006), \"Constructing belief functions from sample data using multinomial confidence regions\",\n",
    "        International Journal of Approximate Reasoning 42, 228-252.\n",
    "        \"\"\"\n",
    "        p_lower, p_upper = MassFunction._confidence_intervals(histogram, alpha)\n",
    "        def p_lower_set(hs):\n",
    "            l = u = 0\n",
    "            for h in H:\n",
    "                if h in hs:\n",
    "                    l += p_lower[h]\n",
    "                else:\n",
    "                    u += p_upper[h]\n",
    "            return max(l, 1 - u)\n",
    "        if ordered:\n",
    "            H = sorted(histogram.keys())\n",
    "            m = MassFunction()\n",
    "            for i1, h1 in enumerate(H):\n",
    "                m[(h1,)] = p_lower[h1]\n",
    "                for i2, h2 in enumerate(H[i1 + 1:]):\n",
    "                    i2 += i1 + 1\n",
    "                    if i2 == i1 + 1:\n",
    "                        v = p_lower_set(H[i1:i2 + 1]) - p_lower[h1] - p_lower[h2]\n",
    "                    else:\n",
    "                        v = p_lower_set(H[i1:i2 + 1]) - p_lower_set(H[i1 + 1:i2 + 1]) - p_lower_set(H[i1:i2]) + p_lower_set(H[i1 + 1:i2])\n",
    "                    if v > 0:\n",
    "                        m[H[i1:i2 + 1]] = v\n",
    "            return m\n",
    "        else:\n",
    "            H = list(histogram.keys())\n",
    "            L = 2**len(H)\n",
    "            initial = numpy.zeros(L)\n",
    "            cons = []\n",
    "            singletons = lambda index: [i for i in range(len(H)) if 2**i & index]\n",
    "            # constraint (24)\n",
    "            bel = lambda index, m: fsum(m[sum([2**i for i in h_ind])] for h_ind in powerset(singletons(index)))\n",
    "            c24 = lambda m, i: p_lower_set(MassFunction._from_array_index(i, H)) - bel(i, m)\n",
    "            for i in range(L):\n",
    "                cons.append(partial(c24, i=i))\n",
    "            # constraint (25)\n",
    "            cons.append(lambda m: m.sum() - 1.0)\n",
    "            cons.append(lambda m: 1.0 - m.sum())\n",
    "            # constraint (26)\n",
    "            for i in range(L):\n",
    "                cons.append(partial(lambda m, i_s: m[i_s], i_s=i))\n",
    "            f = lambda m: -1 * 2**len(H) * fsum([m[i] * 2**(-len(singletons(i))) for i in range(L)])\n",
    "            m_optimal = fmin_cobyla(f, initial, cons, disp=0)\n",
    "            return MassFunction.from_array(m_optimal, H)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _from_samples_mcd(histogram, alpha, approximate=False):\n",
    "        \"\"\"\n",
    "        Reference:\n",
    "        A. Aregui, T. Denoeux (2008), \"Constructing consonant belief functions from sample data using confidence\n",
    "        sets of pignistic probabilities\", International Journal of Approximate Reasoning 49, 575-594.\n",
    "        \"\"\"\n",
    "        p_lower, p_upper = MassFunction._confidence_intervals(histogram, alpha)\n",
    "        H = list(histogram.keys())\n",
    "        if approximate:\n",
    "            # approximate possibility distribution\n",
    "            poss = {h1:min(1, fsum([min(p_upper[h1], p_upper[h2]) for h2 in H])) for h1 in H}\n",
    "        else:\n",
    "            # optimal possibility distribution (based on linear programming)\n",
    "            poss = {h:0. for h in H}\n",
    "            for k, h_k in enumerate(H):\n",
    "                S_k = {l for l in range(len(H)) if p_lower[H[l]] >= p_upper[h_k]}\n",
    "                S_k.add(k)\n",
    "                I_k = {l for l in range(len(H)) if p_upper[H[l]] < p_lower[h_k]}\n",
    "                P_k = set(range(len(H))).difference(S_k.union(I_k))\n",
    "                for A in powerset(P_k):\n",
    "                    G = S_k.union(A)\n",
    "                    G_c = set(range(len(H))).difference(G)\n",
    "                    cons = []\n",
    "                    # constraint (26)\n",
    "                    for i, h in enumerate(H):\n",
    "                        cons.append(partial(lambda p, i_s, p_s: p[i_s] - p_s, i_s=i, p_s=p_lower[h])) # lower bound\n",
    "                        cons.append(partial(lambda p, i_s, p_s: p_s - p[i_s], i_s=i, p_s=p_upper[h])) # upper bound\n",
    "                    # constraint (27)\n",
    "                    cons.append(lambda p: 1. - sum(p))\n",
    "                    cons.append(lambda p: sum(p) - 1.)\n",
    "                    # constraint (30)\n",
    "                    for i in G:\n",
    "                        cons.append(partial(lambda p, i_s: p[i_s] - p[k], i_s=i))\n",
    "                    # constraint (31)\n",
    "                    for i in G_c:\n",
    "                        cons.append(partial(lambda p, i_s: p[k] - p[i_s], i_s=i))\n",
    "                    initial = [1.0 / len(H)] * len(H)\n",
    "                    f = lambda p: -(fsum([p[i] for i in G_c]) + len(G) * p[k])\n",
    "                    poss_optimal = fmin_cobyla(f, initial, cons, disp=0)\n",
    "                    poss[h_k] = max(poss[h_k], -f(poss_optimal))\n",
    "        return MassFunction.from_possibility(poss)\n",
    "\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"\"\"\n",
    "    Returns an iterator over the power set of 'set'.\n",
    "    \n",
    "    'set' is an arbitrary iterator over hashable elements.\n",
    "    All returned subsets are of type 'frozenset'.\n",
    "    \"\"\"\n",
    "    return map(frozenset, chain.from_iterable(combinations(iterable, r) for r in range(len(iterable) + 1)))\n",
    "\n",
    "def gbt_m(hypothesis, likelihoods, normalization=True):\n",
    "    \"\"\"\n",
    "    Computes the mass value of 'hypothesis' using the generalized Bayesian theorem.\n",
    "    \n",
    "    Equivalent to MassFunction.gbt(likelihoods, normalization)[hypothesis].\n",
    "    \"\"\"\n",
    "    if isinstance(likelihoods, dict):\n",
    "        likelihoods = list(likelihoods.items())\n",
    "    q = gbt_q(hypothesis, likelihoods, normalization)\n",
    "    return q * reduce(mul, [1.0 - l[1] for l in likelihoods if l[0] not in hypothesis], 1.0)\n",
    "\n",
    "def gbt_bel(hypothesis, likelihoods, normalization=True):\n",
    "    \"\"\"\n",
    "    Computes the belief of 'hypothesis' using the generalized Bayesian theorem.\n",
    "    \n",
    "    Equivalent to MassFunction.gbt(likelihoods, normalization).bel(hypothesis).\n",
    "    \"\"\"\n",
    "    if isinstance(likelihoods, dict):\n",
    "        likelihoods = list(likelihoods.items())\n",
    "    eta = _gbt_normalization(likelihoods) if normalization else 1.0\n",
    "    exc = reduce(mul, [1.0 - l[1] for l in likelihoods if l[0] not in hypothesis], 1.0)\n",
    "    all_hyp = reduce(mul, [1.0 - l[1] for l in likelihoods], 1.0)\n",
    "    return eta * (exc - all_hyp)\n",
    "\n",
    "def gbt_pl(hypothesis, likelihoods, normalization=True):\n",
    "    \"\"\"\n",
    "    Computes the plausibility of 'hypothesis' using the generalized Bayesian theorem.\n",
    "    \n",
    "    Equivalent to MassFunction.gbt(likelihoods, normalization).pl(hypothesis).\n",
    "    \"\"\"\n",
    "    if isinstance(likelihoods, dict):\n",
    "        likelihoods = list(likelihoods.items())\n",
    "    eta = _gbt_normalization(likelihoods) if normalization else 1.0\n",
    "    return eta * (1.0 - reduce(mul, [1.0 - l[1] for l in likelihoods if l[0] in hypothesis], 1.0))\n",
    "    \n",
    "def gbt_q(hypothesis, likelihoods, normalization=True):\n",
    "    \"\"\"\n",
    "    Computes the commonality of 'hypothesis' using the generalized Bayesian theorem.\n",
    "    \n",
    "    Equivalent to MassFunction.gbt(likelihoods, normalization).q(hypothesis).\n",
    "    \"\"\"\n",
    "    if isinstance(likelihoods, dict):\n",
    "        likelihoods = list(likelihoods.items())\n",
    "    eta = _gbt_normalization(likelihoods) if normalization else 1.0\n",
    "    return eta * reduce(mul, [l[1] for l in likelihoods if l[0] in hypothesis], 1.0)\n",
    "\n",
    "def gbt_pignistic(singleton, likelihoods):\n",
    "    \"\"\"\n",
    "    Computes the pignistic probability of 'singleton' for the belief function obtained\n",
    "    by applying the generalized Bayesian theorem to 'likelihoods'.\n",
    "    \n",
    "    This function has time complexity O(len(likelihoods)**2) and is equivalent to the following\n",
    "    expression (which has exponential complexity):\n",
    "    MassFunction.gbt(likelihoods).pignistic()[(singleton,)]\n",
    "    \"\"\"\n",
    "    if isinstance(likelihoods, dict):\n",
    "        likelihoods = list(likelihoods.items())\n",
    "    singleton_lh = None\n",
    "    lh_values = []\n",
    "    for h, v in likelihoods:\n",
    "        if h == singleton:\n",
    "            singleton_lh = v\n",
    "        else:\n",
    "            lh_values.append(v)\n",
    "    if singleton_lh is None:\n",
    "        raise ValueError('singleton %s is not contained in likelihoods' % repr(singleton))\n",
    "    m_sum = _gbt_pignistic_recursive(lh_values, 0)\n",
    "    eta = _gbt_normalization(likelihoods)\n",
    "    return sum([eta * v * singleton_lh / (c + 1.) for c, v in enumerate(m_sum)])\n",
    "\n",
    "def _gbt_pignistic_recursive(likelihoods, i):\n",
    "    \"\"\"\n",
    "    Helper function for recursively computing the pignistic probability corresponding to the GBT.\n",
    "    \n",
    "    This function computes the sum over all mass values (obtained via the GBT) and groups them by the\n",
    "    cardinalities of the corresponding sets.\n",
    "    \"\"\"\n",
    "    if i == len(likelihoods) - 1:\n",
    "        m_sum = [0.] * (len(likelihoods) + 1)\n",
    "        m_sum[0] = 1. - likelihoods[i]\n",
    "        m_sum[1] = likelihoods[i]\n",
    "        return m_sum\n",
    "    else:\n",
    "        m_sum = _gbt_pignistic_recursive(likelihoods, i + 1)\n",
    "        m_sum_inc = [0.] * (len(likelihoods) + 1)\n",
    "        m_sum_exc = [0.] * (len(likelihoods) + 1)\n",
    "        for k in range(len(likelihoods) + 1):\n",
    "            if k < len(likelihoods):\n",
    "                m_sum_inc[k+1] = m_sum[k] * likelihoods[i]\n",
    "            m_sum_exc[k] = m_sum[k] * (1. - likelihoods[i])\n",
    "            m_sum[k] = m_sum_inc[k] + m_sum_exc[k]\n",
    "        return m_sum\n",
    "\n",
    "def _gbt_normalization(likelihoods):\n",
    "    \"\"\"Helper function for computing the GBT normalization constant.\"\"\"\n",
    "    return 1.0 / (1.0 - reduce(mul, [1.0 - l[1] for l in likelihoods], 1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b3f596",
   "metadata": {},
   "source": [
    "# Naive Adaptive Sensor Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67bef265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Oct 29 18:13:08 2019\n",
    "\n",
    "@author: 14342\n",
    "\"\"\"\n",
    "from pyds_local import *\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import clone\n",
    "import itertools\n",
    "import sys\n",
    "import re\n",
    "\n",
    "class DS_Model:\n",
    "    \n",
    "    def __init__(self, resp_var_1, resp_var_2, X_train, y_train, feature_set_idx):\n",
    "        \"\"\"\n",
    "        Initializing a DS model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        resp_var_1 : list\n",
    "        resp_var_2 : list\n",
    "            These two are the response variables that we want to build a model\n",
    "            on. For example if we are building a model to classify {c1} vs {c2,c3}\n",
    "            then [c1] would be our resp_var_1 and [c2,c3] would be our resp_var_2.\n",
    "            \n",
    "        X_train : pd.dataframe\n",
    "            training features in a dataframe format.\n",
    "        y_train : pd.dataframe\n",
    "            training labels as a vector.\n",
    "        feature_set_idx : integer\n",
    "            Index of the feature set (out of all the randomly created feature sets).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.Feature_Set_idx = feature_set_idx\n",
    "        self.Response_Variables = [resp_var_1,resp_var_2]\n",
    "        self.clf = LogisticRegression(class_weight='balanced',solver='saga', n_jobs=-1).fit(X_train, y_train)\n",
    "        self.Bags = []                          #list of the bags for this model\n",
    "        self.Uncertainty_B = 0                  #Uncertainty of the biased model\n",
    "        self.mass = MassFunction()              #Mass function of the model\n",
    "    \n",
    "    def Mass_Function_Setter(self, uncertainty, X):\n",
    "        \"\"\"\n",
    "        We used pyds package (a dempster shafer package) to define the mass function\n",
    "        given the probabilities and uncertainty.\n",
    "        \"\"\"\n",
    "        probability = self.clf.predict_proba(X)\n",
    "        self.mass[self.Response_Variables[0]] = probability[0,0]*(1-uncertainty)\n",
    "        self.mass[self.Response_Variables[1]] = probability[0,1]*(1-uncertainty)\n",
    "    \n",
    "    def Bags_Trainer(self, X_train, y_train, ratio, num_bags):\n",
    "        \"\"\"\n",
    "        This function trains bags\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : pd.dataframe\n",
    "            training features in a dataframe format.\n",
    "        y_train : pd.dataframe\n",
    "            training labels as a vector.\n",
    "        ratio : float\n",
    "            Ratio of the generated bagging size to the actual training dataset.\n",
    "        num_bags : integer\n",
    "            number of the bags to generate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        for i in range(num_bags):\n",
    "            self.Bags.append(clone(self.clf))\n",
    "            indices = random.choices(list(range(len(X_train))), k=int(ratio*(len(X_train))))\n",
    "            X_train_Bag = X_train.iloc[indices,:]\n",
    "            y_train_Bag = y_train.iloc[indices]\n",
    "            \n",
    "            self.Bags[i].fit(X_train_Bag, y_train_Bag)\n",
    "    \n",
    "    def Uncertainty_Context(self, X_test_single):\n",
    "        \"\"\"\n",
    "        This function calculates the uncertainty of the contextual meaning by\n",
    "        calculating the votes from all the bags.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_test_single : pd.series\n",
    "            one single test example.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        C = len(self.Response_Variables) #Number of the classes\n",
    "        V = np.zeros(C) #Vote vector\n",
    "        T = len(self.Bags) #total number of the bags\n",
    "        \n",
    "        for i in range(T):\n",
    "            V[int(self.Bags[i].predict(X_test_single))] += 1\n",
    "        \n",
    "        Uncertainty_c = 1 - np.sqrt(np.sum(np.power((V/T-1/C),2)))/np.sqrt((C-1)/C)\n",
    "        return(Uncertainty_c)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "class ProgressBar(object):\n",
    "    DEFAULT = 'Progress: %(bar)s %(percent)3d%%'\n",
    "    FULL = '%(bar)s %(current)d/%(total)d (%(percent)3d%%) %(remaining)d remaining feature sets'\n",
    "\n",
    "    def __init__(self, total, width=40, fmt=DEFAULT, symbol='=',\n",
    "                 output=sys.stderr):\n",
    "        assert len(symbol) == 1\n",
    "\n",
    "        self.total = total\n",
    "        self.width = width\n",
    "        self.symbol = symbol\n",
    "        self.output = output\n",
    "        self.fmt = re.sub(r'(?P<name>%\\(.+?\\))d',\n",
    "            r'\\g<name>%dd' % len(str(total)), fmt)\n",
    "\n",
    "        self.current = 0\n",
    "\n",
    "    def __call__(self):\n",
    "        percent = self.current / float(self.total)\n",
    "        size = int(self.width * percent)\n",
    "        remaining = self.total - self.current\n",
    "        bar = '[' + self.symbol * size + ' ' * (self.width - size) + ']'\n",
    "\n",
    "        args = {\n",
    "            'total': self.total,\n",
    "            'bar': bar,\n",
    "            'current': self.current,\n",
    "            'percent': percent * 100,\n",
    "            'remaining': remaining\n",
    "        }\n",
    "        print('\\r' + self.fmt % args, file=self.output, end='')\n",
    "\n",
    "    def done(self):\n",
    "        self.current = self.total\n",
    "        self()\n",
    "        print('', file=self.output)\n",
    "\n",
    "\n",
    "def feature_set(sensors_to_fuse, st_feat, Sensors, feat_set_count=100):\n",
    "    \"\"\"\n",
    "    This function takes a list of name of the sensors to fuse \"sensors_to_fuse\n",
    "    and structure of the reduced feature space also as a list (like [s1,s2,..])\n",
    "    where \"si\" is the number of the features of the ith sensor to be used and \n",
    "    also number of the reduced feature sets to create. Then it returns a matrix\n",
    "    with each row being a feature set number of columns\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    sensors_to_fuse: list\n",
    "        a list of the name of the sensors to fuse. like ['Acc','Gyro','PS','Aud']\n",
    "    st_feat: list\n",
    "        the structure of a feature set. like [s1,s2,..] where \"si\" is \n",
    "        the number of the features of the ith sensor\n",
    "    feat_set_count: integer\n",
    "        the number of randome feature sets to create. It is a number like 100\n",
    "    Sensors: dict\n",
    "        the sensors dictionary which is the output of the labeling function\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "        selected_feats: np.array 2D\n",
    "            a matrix that each row represents one set of features and the \n",
    "            values in each rows are the index of the columns of the data to\n",
    "            be used as features\n",
    "       \n",
    "    \"\"\"\n",
    "\n",
    "    #Making sure that the length of the \"st_feat\" is equal to the length of the\n",
    "    #\"sensors_to_fuse\n",
    "    assert len(st_feat) == len(sensors_to_fuse)\n",
    "    \n",
    "    #Creating and initializing the \"selected_feats\" to 0\n",
    "    selected_feats = np.zeros([feat_set_count,sum(st_feat)])\n",
    "    \n",
    "    #A for loop to create the desired number of the random reduced feature sets\n",
    "    for j in range(feat_set_count):\n",
    "        \n",
    "        #\"col\" stores the index of the randomly generated columns (features)\n",
    "        col = []\n",
    "        for i in range(len(st_feat)):\n",
    "            \n",
    "            #\"aux\" stores the index of the generated columns of one sensor\n",
    "            aux = random.sample(Sensors[sensors_to_fuse[i]],st_feat[i])\n",
    "            aux.sort()\n",
    "            col.extend(aux)\n",
    "        selected_feats[j,:] = col\n",
    "    return(selected_feats)\n",
    "\n",
    "\n",
    "def BPA_builder(labels):\n",
    "    \"\"\"This function creates the Basic Probability Assignment (BPA) matrix given\n",
    "       a list of labels (classes)\n",
    "       \n",
    "       Input:\n",
    "           labels: a list of the target labels to be used as our propositins.\n",
    "                   In other words it is the Frame of discernment (FOD).\n",
    "                   like: ['Walking','Lying_down']\n",
    "       Ouput:\n",
    "           mass: a dataframe that has all the subsets of the the FOD. each row\n",
    "                 respresent a sebset with a binary vector like:\n",
    "                     \n",
    "                     Lying_down     Sleeping    mass\n",
    "                     0              0           0\n",
    "                     0              1           0\n",
    "                     1              0           0\n",
    "                     1              1           0\n",
    "                 \n",
    "                 ex. the last row represents {'Lying_down','Sleeping'} and m is\n",
    "                 the corresponding basic beleif assignment or mass function.\n",
    "                 also all masses are initialized with 0\n",
    "                   \n",
    "    \"\"\"\n",
    "       \n",
    "    mass = pd.DataFrame(columns=labels)\n",
    "    perms = list(itertools.product([0, 1], repeat=len(labels)))\n",
    "    for i in range(len(perms)):\n",
    "        mass.loc[i] = perms[i]\n",
    "    mass['m'] = 0\n",
    "    return(mass)\n",
    "    \n",
    "    \n",
    "def pair_resp(BPA):\n",
    "    \"\"\"This function takes in the mss function (dataframe in fact) and returns\n",
    "       two lists. Each element in each of the lists is itself a list of labels\n",
    "       representing one subset of the FOD or one member of the power set. Note\n",
    "       that perms_set_1[i] and perms_set_2[i] are complementary subsets.\n",
    "       \n",
    "       Input:\n",
    "           BPA: the mass dataframe. output of the BPA_builder()\n",
    "       \n",
    "       Output:\n",
    "           perms_set_1: a list of lists\n",
    "           perms_set_2: the same\n",
    "           \n",
    "       ex. target labels = 'Lying_down','Sleeping','Walking'\n",
    "           perms_set_1[2]=['Lying_down','Sleeping'] \n",
    "           perms_set_2[2]=['Walking']\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    perms_set_1 = []\n",
    "    perms_set_2 = []\n",
    "    \n",
    "    #Here we don't need the last column of the mass which is the valuses of mass\n",
    "    #function. So we get rid of them so that they won't interfere later\n",
    "    BPA = BPA.iloc[:,:-1]\n",
    "    \n",
    "    #It is enough to go up to the half of the rows of the mass to generate the\n",
    "    #two classes of permustations. The rest are just the complementary to the\n",
    "    #first half. In fact if the powerset has 2**n elements, we have 2**(n-1) \n",
    "    #complementary pairs\n",
    "    for i in range(BPA.shape[0]//2):\n",
    "        assert (sum(BPA.loc[i,:])+sum(BPA.loc[BPA.shape[0]-i-1,:])) == BPA.shape[1]\n",
    "        perms_set_1.append([])\n",
    "        perms_set_2.append([])\n",
    "        \n",
    "        for j in range(BPA.shape[1]):\n",
    "            if BPA.iloc[i,j] == 1:\n",
    "                perms_set_1[i].append(BPA.columns[j])\n",
    "            else:\n",
    "                perms_set_2[i].append(BPA.columns[j])\n",
    "    \n",
    "    perms_set_1.remove(perms_set_1[0])\n",
    "    perms_set_2.remove(perms_set_2[0])\n",
    "    \n",
    "    return(perms_set_1,perms_set_2)\n",
    "\n",
    "\n",
    "def Uncertainty_Bias(response_variables):\n",
    "    \"\"\"\n",
    "    This function calculates the \"uncertainty of the biased model\" based on the\n",
    "    frequency of the classes.\n",
    "    \n",
    "    Input:\n",
    "        response_variables[list of pd.Dataframe]: each element of the list is \n",
    "        itself a dataframe of one class (response variable).\n",
    "    \n",
    "    Output:\n",
    "        U[float]: uncertainty of the biased model\n",
    "    \"\"\"\n",
    "    \n",
    "    C = len(response_variables) #number of the classes (response variables)\n",
    "    I = np.zeros(C) #an array of the frequencies of the classes\n",
    "    \n",
    "    S = 0 #total samples in the dataset\n",
    "    for i in range(C):\n",
    "        I[i] = int(np.sum(response_variables[i] == 1))\n",
    "        S += I[i]\n",
    "    \n",
    "    U = np.sqrt(np.sum(np.power((I/S-1/C),2)))/np.sqrt((C-1)/C)\n",
    "    return (U)\n",
    "\n",
    "    \n",
    "def Model_Selector(uncertainty_m, models_per_rp, num_rp, fs_axis):\n",
    "    \"\"\"This function takes the total uncertainty matrix and chooses the least\n",
    "    uncertain models for every response permutation.\n",
    "    \n",
    "    Input:\n",
    "        uncertainty_m[np.array 2d]: Matrix of the total uncertainty for all models.\n",
    "        models_per_rp[int]: Number of the models to select for each response permutation\n",
    "        num_rp[int]: Number of the response variables (Also the length of the \n",
    "              uncertainty matrix along one of the dimensions)\n",
    "        fs_axis[int]: The axis of the uncertainty matrix along which feature sets\n",
    "            are laid\n",
    "    \n",
    "    Output:\n",
    "        selected_models_idx[np.array 2d]: A 2d array that holds the feature set\n",
    "            indices for each response permutation\n",
    "    \"\"\"\n",
    "    \n",
    "    selected_models_idx = np.zeros([num_rp, models_per_rp])\n",
    "    \n",
    "    index_m = np.argsort(uncertainty_m, axis= fs_axis)\n",
    "    \n",
    "    for i in range(models_per_rp):\n",
    "        selected_models_idx[:,i] = np.argwhere(index_m == i)[:,1]\n",
    "    \n",
    "    return(selected_models_idx)\n",
    "        \n",
    "def Fuse_and_Predict(selected_models_idx, Models, FOD, num_classes, num_rp, models_per_rp):\n",
    "    assert len(selected_models_idx) == num_rp\n",
    "    y_pred = np.zeros([1,num_classes])\n",
    "    \n",
    "    fs_idx = int(selected_models_idx[0][0])\n",
    "    combined_mass = Models[0][fs_idx].mass\n",
    "    for i in range(num_rp):\n",
    "        for j in range(models_per_rp):\n",
    "            if i == 0 and j == 0:\n",
    "                continue\n",
    "            else:\n",
    "                fs_idx = int(selected_models_idx[i][j])\n",
    "                combined_mass = combined_mass & Models[i][fs_idx].mass\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        y_pred[0,i] = combined_mass[{FOD[i]}]\n",
    "    \n",
    "    y_pred_aux = np.zeros_like(y_pred)\n",
    "    y_pred_aux[np.arange(len(y_pred)), y_pred.argmax(1)] = 1\n",
    "    y_pred = y_pred_aux\n",
    "    \n",
    "    return(y_pred)\n",
    "    \n",
    "# def NAPS_Models_Trainer(num_rp, fs_range, train_dataset, feature_sets, Response_Perm_1, \\\n",
    "#                         Response_Perm_2, impute = True):\n",
    "#     \"\"\"\n",
    "#     This function trains NAPS models.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     num_rp : int\n",
    "#         number of the response permutations (which is 2**(n-1)-1 for n classes).\n",
    "#     fs_range : range or list\n",
    "#         a list of int from 0 to m (m being the total number of the feature sets).\n",
    "#     train_dataset : pd.dataframe\n",
    "#         dataframe of the training features.\n",
    "#     feature_sets : np.array 2D\n",
    "#         matrix of the selected features.\n",
    "#     Response_Perm_1 : list\n",
    "#         lsit of the response permutations.\n",
    "#     Response_Perm_2 : list\n",
    "#         complementary list of the Response_Perm_1.\n",
    "#     impute : bool, optional\n",
    "#         Whether to impute or discard missing features. The default is True.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     None.\n",
    "\n",
    "#     \"\"\"\n",
    "#     NAPS_models = []\n",
    "#     print('\\nLooping over Response Permutations \\n ')\n",
    "    \n",
    "#     for i in range(num_rp):\n",
    "#         print('\\nResponse Permutation {}/{}'.format(i+1,num_rp))\n",
    "#         print('\\n\\tLooping over feature sets')\n",
    "#         NAPS_models.append([])\n",
    "                \n",
    "#         for j in fs_range:\n",
    "#             print('\\t\\tFeature Set {}/{}'.format(j+1, len(fs_range)))\n",
    "#             NAPS_models[i].append([])\n",
    "#             #find X and y\n",
    "#             X_train, y_train, y1, y2 = Xy(train_dataset, feature_sets, j, \\\n",
    "#                             Response_Perm_1, Response_Perm_2, i, impute = True)\n",
    "#     #        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "            \n",
    "#             X_train_tmp, y_train_tmp = smt.fit_sample(X_train, y_train)\n",
    "            \n",
    "#             X_train = pd.DataFrame(X_train_tmp, columns = X_train.columns)\n",
    "#             y_train = pd.DataFrame(y_train_tmp, columns = ['Merged_label'])\n",
    "            \n",
    "            \n",
    "#             if j == 0:\n",
    "#                 U2 = Uncertainty_Bias([y1, y2])\n",
    "    \n",
    "#             #create a model and train it\n",
    "#             NAPS_models[i][j] = DS_Model(Response_Perm_1[i], Response_Perm_2[i], \\\n",
    "#                        X_train, y_train, j)\n",
    "#             NAPS_models[i][j].Bags_Trainer(X_train, y_train, bagging_R, num_bags)\n",
    "#             NAPS_models[i][j].Uncertainty_B = U2\n",
    "    \n",
    "#     return(NAPS_models)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a7be4",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c581362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jun 15 21:05:15 2020\n",
    "\n",
    "@author: 14342\n",
    "\"\"\"\n",
    "data_dir = \"C:\\\\Users\\\\mehrd\\\\OneDrive\\\\Documents\\\\GitHub\\\\NAPS-Fusion\\\\datasets\"\n",
    "cvdir = \"C:\\\\Users\\\\mehrd\\\\OneDrive\\\\Documents\\\\GitHub\\\\NAPS-Fusion\\\\cv5Folds\\\\cv_5_folds\\\\\"\n",
    "\n",
    "sensors_to_fuse = ['Acc','Gyro','W_acc','Aud'] \n",
    "#there are also \"Mag\", \"Loc\", \"AP\", \"PS\", \"LF\", \"Compass\" sensors\n",
    "FOD = ['label:LYING_DOWN','label:SITTING','label:OR_standing','label:FIX_walking']#\\\n",
    "#      ,'label:FIX_running','label:BICYCLING']\n",
    "feature_sets_st = [3,3,3,3] #feature set structure\n",
    "feature_sets_count = 10\n",
    "bagging_R = 0.6  #bagging ratio\n",
    "num_bags = 4\n",
    "models_per_rp = 2  #number of models to select for the fusion per response permutation\n",
    "feature_range = range(1,225) #range of the column number of all features\n",
    "num_prc = 6  #number of processors to split the job during parallelization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e34311",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99c91859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#------------- Reading in the Data of Users -------------#\n",
      "\n",
      "User 1/60  -> Shape of the data     (2287, 278)\n",
      "User 2/60  -> Shape of the data     (6813, 278)\n",
      "User 3/60  -> Shape of the data     (3960, 278)\n",
      "User 4/60  -> Shape of the data     (3108, 278)\n",
      "User 5/60  -> Shape of the data     (7521, 278)\n",
      "User 6/60  -> Shape of the data     (2685, 278)\n",
      "User 7/60  -> Shape of the data     (8845, 278)\n",
      "User 8/60  -> Shape of the data     (6218, 278)\n",
      "User 9/60  -> Shape of the data     (6549, 278)\n",
      "User 10/60  -> Shape of the data     (7375, 278)\n",
      "User 11/60  -> Shape of the data     (4771, 278)\n",
      "User 12/60  -> Shape of the data     (4927, 278)\n",
      "User 13/60  -> Shape of the data     (8516, 278)\n",
      "User 14/60  -> Shape of the data     (6172, 278)\n",
      "User 15/60  -> Shape of the data     (5203, 278)\n",
      "User 16/60  -> Shape of the data     (7649, 278)\n",
      "User 17/60  -> Shape of the data     (6691, 278)\n",
      "User 18/60  -> Shape of the data     (3250, 278)\n",
      "User 19/60  -> Shape of the data     (4979, 278)\n",
      "User 20/60  -> Shape of the data     (6617, 278)\n",
      "User 21/60  -> Shape of the data     (6617, 278)\n",
      "User 22/60  -> Shape of the data     (5947, 278)\n",
      "User 23/60  -> Shape of the data     (7542, 278)\n",
      "User 24/60  -> Shape of the data     (3911, 278)\n",
      "User 25/60  -> Shape of the data     (6079, 278)\n",
      "User 26/60  -> Shape of the data     (8730, 278)\n",
      "User 27/60  -> Shape of the data     (9167, 278)\n",
      "User 28/60  -> Shape of the data     (7298, 278)\n",
      "User 29/60  -> Shape of the data     (11996, 278)\n",
      "User 30/60  -> Shape of the data     (3593, 278)\n",
      "User 31/60  -> Shape of the data     (9761, 278)\n",
      "User 32/60  -> Shape of the data     (1600, 278)\n",
      "User 33/60  -> Shape of the data     (9189, 278)\n",
      "User 34/60  -> Shape of the data     (9242, 278)\n",
      "User 35/60  -> Shape of the data     (6407, 278)\n",
      "User 36/60  -> Shape of the data     (9539, 278)\n",
      "User 37/60  -> Shape of the data     (10738, 278)\n",
      "User 38/60  -> Shape of the data     (5819, 278)\n",
      "User 39/60  -> Shape of the data     (9959, 278)\n",
      "User 40/60  -> Shape of the data     (6038, 278)\n",
      "User 41/60  -> Shape of the data     (9686, 278)\n",
      "User 42/60  -> Shape of the data     (1667, 278)\n",
      "User 43/60  -> Shape of the data     (6040, 278)\n",
      "User 44/60  -> Shape of the data     (3898, 278)\n",
      "User 45/60  -> Shape of the data     (7520, 278)\n",
      "User 46/60  -> Shape of the data     (8134, 278)\n",
      "User 47/60  -> Shape of the data     (9383, 278)\n",
      "User 48/60  -> Shape of the data     (7626, 278)\n",
      "User 49/60  -> Shape of the data     (8309, 278)\n",
      "User 50/60  -> Shape of the data     (3451, 278)\n",
      "User 51/60  -> Shape of the data     (5092, 278)\n",
      "User 52/60  -> Shape of the data     (7865, 278)\n",
      "User 53/60  -> Shape of the data     (8472, 278)\n",
      "User 54/60  -> Shape of the data     (2860, 278)\n",
      "User 55/60  -> Shape of the data     (3615, 278)\n",
      "User 56/60  -> Shape of the data     (6210, 278)\n",
      "User 57/60  -> Shape of the data     (3441, 278)\n",
      "User 58/60  -> Shape of the data     (3530, 278)\n",
      "User 59/60  -> Shape of the data     (2266, 278)\n",
      "User 60/60  -> Shape of the data     (4973, 278)\n",
      "Reading the data took:    40\n",
      "\n",
      "#-------------- Combining the Data of Users-------------#\n",
      "\n",
      "Combining the data took:    87\n",
      "\n",
      "#-------------- Obtaining Training Dataset -------------#\n",
      "\n",
      "Obtaining the training dataset took:    19\n",
      "Training dataset has  312287  samples\n",
      "\n",
      "#-------------- Creating and Training Models -------------#\n",
      "\n",
      "\n",
      "Looping over Response Permutations \n",
      " \n",
      "\n",
      "Response Permutation 1/7\n",
      "\n",
      "\tLooping over feature sets\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a64c456c8449>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[0mNAPS_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m                 train_NAPS_Models(train_dataset, feature_sets, j, Response_Perm_1, \\\n\u001b[1;32m--> 432\u001b[1;33m                           Response_Perm_2, i, bagging_R, num_bags, impute = True)\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[1;31m#find X and y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;31m#         X_train, y_train, y1, y2 = Xy(train_dataset, feature_sets, j, \\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-a64c456c8449>\u001b[0m in \u001b[0;36mtrain_NAPS_Models\u001b[1;34m(train_dataset, feature_sets, j, Response_Perm_1, Response_Perm_2, i, bagging_R, num_bags, impute)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;31m# print('\\tCreating the DS model took : ', t4-t3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m     \u001b[0mNAPS_sample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBags_Trainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbagging_R\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_bags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m     \u001b[0mt5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[1;31m# print('\\tTraining bags took : ', t5-t4)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\NAPS-Fusion\\Naive_Adaptive_Sensor_Fusion.py\u001b[0m in \u001b[0;36mBags_Trainer\u001b[1;34m(self, X_train, y_train, ratio, num_bags)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0my_train_Bag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBags\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_Bag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_Bag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mUncertainty_Context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_single\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NAPS\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1414\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1415\u001b[0m                       sample_weight=sample_weight)\n\u001b[1;32m-> 1416\u001b[1;33m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[0;32m   1417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1418\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NAPS\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NAPS\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NAPS\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NAPS\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NAPS\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NAPS\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Oct 30 13:17:55 2019\n",
    "\n",
    "@author: 14342\n",
    "\"\"\"\n",
    "from __future__ import absolute_import, division, print_function\n",
    "from pyds_local import *\n",
    "from Naive_Adaptive_Sensor_Fusion import *\n",
    "from config import *\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import timeit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "#import tensorflow as tf\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def readdata_csv(data_dir):\n",
    "    \"\"\"This function gets the directory of the datasets and returns the dataset\n",
    "    containing information of all 60 users\n",
    "    \n",
    "    Input:\n",
    "        data_dir[string]: holds the directory of all the csv files (60)\n",
    "        \n",
    "    Output:\n",
    "        grand_dataset[dict]: a dictionary of all the users' data. The format is:\n",
    "            grand_dataset:{'uuid1': dataframe of csv file of the user 1\n",
    "                           'uuid2': dataframe of csv file of the user 2\n",
    "                           'uuid3': dataframe of csv file of the user 3\n",
    "                           ...}\n",
    "    \"\"\"\n",
    "    length_uuids = 36 # number of characters for each uuid\n",
    "    data_list = glob.glob(os.path.join(os.getcwd(), data_dir, \"*.csv\"))\n",
    "    # grand_dataset is a dict. that holds the uuids and correspondong datast\n",
    "    grand_dataset = {}\n",
    "    for i in range(len(data_list)):\n",
    "#    for i in range(5):\n",
    "        # dismantles the file name and picks only uuids (first 36 characters)\n",
    "        uuid = os.path.basename(data_list[i])[:length_uuids]    \n",
    "        dataset_ith = pd.read_csv(data_list[i])\n",
    "        print('User {}/{}  -> Shape of the data     {}'.format(i+1, \\\n",
    "              len(data_list), dataset_ith.shape))\n",
    "        grand_dataset[uuid] = dataset_ith\n",
    "    return(grand_dataset)\n",
    "    \n",
    "def Set_Act_Sens():\n",
    "    \"\"\"This function defines two dictionaries for activities and sensors. Each\n",
    "    dictionaray holds the the range of columns for the specified sensor or \n",
    "    activity.\n",
    "    \n",
    "    Input:\n",
    "    Output:\n",
    "        Activities[dict]: a dictionary of the activities and their corresponding\n",
    "                        column number\n",
    "        Sensors[dict]: a dictionary of the sensors and their corresponding range\n",
    "                        of features\n",
    "    \"\"\"\n",
    "    Activities = {}\n",
    "    Activities['label:LYING_DOWN'] = 226\n",
    "    Activities['label:SITTING'] = 227\n",
    "    Activities['label:FIX_walking'] = 228\n",
    "    Activities['label:FIX_running'] = 229\n",
    "    Activities['label:BICYCLING'] = 230\n",
    "    Activities['label:SLEEPING'] = 231\n",
    "    Activities['label:OR_standing'] = 270\n",
    "    \n",
    "    \n",
    "    Sensors = {}\n",
    "    Sensors['Acc'] = list(range(1,27))\n",
    "    Sensors['Gyro'] = list(range(27,53))\n",
    "#    Sensors['Mag'] = list(range(53,84))\n",
    "    Sensors['W_acc'] = list(range(84,130))\n",
    "#    Sensors['Compass'] = list(range(130,139))\n",
    "    Sensors['Loc'] = list(range(139,156))\n",
    "    Sensors['Aud'] = list(range(156,182))\n",
    "#    Sensors['AP'] = list(range(182,184))\n",
    "    Sensors['PS'] = list(np.append(range(184,210),range(218,226)))\n",
    "#    Sensors['LF'] = list(range(210,218))\n",
    "    \n",
    "    return(Activities,Sensors)\n",
    "    \n",
    "def Response_Merger(data, cols_to_merge):\n",
    "    \"\"\"\n",
    "    This function takes in the dataset and a list of columns of different labels \n",
    "    to merge and combnine them using a logical OR to give back one column. ex. \n",
    "    l1+l2+l3 -> {l1,l2,l3}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dataframe\n",
    "        dataframe of the dataset (ex. training data).\n",
    "    cols_to_merge : list\n",
    "        a list of the columns to merge with a logical OR. like:\n",
    "        ['Lying_down','Sleeping'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    merged_label: dataframe\n",
    "        a dataframe with only one column whose values are binary\n",
    "\n",
    "    \"\"\"\n",
    "    data = data[cols_to_merge].fillna(0)\n",
    "    \n",
    "    merged_label = data[cols_to_merge[0]]\n",
    "    \n",
    "    for i in range(1,len(cols_to_merge)):\n",
    "        merged_label = np.logical_or(merged_label, data[cols_to_merge[i]])*1\n",
    "    \n",
    "    merged_label = merged_label.to_frame()\n",
    "    merged_label.columns=['Merged_label']\n",
    "\n",
    "     \n",
    "#    col_name = ''.join(cols_to_merge[:])\n",
    "#    cols_to_merge = add_label(cols_to_merge)\n",
    "#        \n",
    "#    # First we impute the NaN with 0 in all the columns that are about to be merged\n",
    "#    data = data[cols_to_merge].fillna(0)\n",
    "#    \n",
    "#    # Now find the logical OR of the desired columns (labels)\n",
    "#    merged_label = data[cols_to_merge[0]]\n",
    "#    merged_label.name = col_name\n",
    "#    \n",
    "#    for i in range(len(cols_to_merge)):\n",
    "#        merged_label = np.logical_or(merged_label, data[cols_to_merge[i]])\n",
    "    return(merged_label)\n",
    "\n",
    "def Xy(data, feature_sets, feature_set_idx, response_perms_1, response_perms_2,\\\n",
    "       response_perm_idx, impute = False):\n",
    "    \"\"\"This function takes data, feature sets matrix, respnse perms, the index\n",
    "       of the row of the desired feature set, and the index of the desired rows\n",
    "       of the response variables and gives back the X and y\"\"\"\n",
    "    # Maybe add sorting later\n",
    "    # Maybe some more manupulations on response variable\n",
    "    X = data.iloc[:,list(feature_sets[feature_set_idx,:])]\n",
    "#    X.loc[:,:] = preprocessing.scale(X)\n",
    "    \n",
    "    if impute is not False:\n",
    "        X = X.fillna(0)\n",
    "    \n",
    "    y1 = Response_Merger(data, response_perms_1[response_perm_idx])\n",
    "    y2 = Response_Merger(data, response_perms_2[response_perm_idx])\n",
    "    \n",
    "    aux = y1 + y2\n",
    "    indices = np.where(aux ==1)[0]\n",
    "    \n",
    "    if len(indices) > 1:\n",
    "        y1 = y1.loc[indices,:]\n",
    "        y2 = y2.loc[indices,:]\n",
    "        X = X.loc[indices,:]\n",
    "    \n",
    "    y = y1\n",
    "    \n",
    "    xy = pd.concat([X,y],axis=1)\n",
    "    xy = xy.dropna()\n",
    "    \n",
    "    y = xy.iloc[:,-1]\n",
    "    X = xy.iloc[:,0:-1]\n",
    "    \n",
    "    return(X, y, y1, y2)\n",
    "\n",
    "def train_test_spl(test_fold, num_folds, fold_dir, grand_dataset):\n",
    "    \"\"\"This function takes the number of test fold (ranging from 0 to 4) and\n",
    "    number of folds (in this case 5) and directory where the folds' uuids are\n",
    "    and the dataset, and returns train and test datasets\n",
    "    \n",
    "    Input:\n",
    "        test_fold_idx[integer]: an integer indicating the index of the test fold\n",
    "        fold_dir[string]: holds the directory in which the folds' uuids are\n",
    "        grand_dataset[dict]: a dictionary of all users' data. (essentially the\n",
    "                             output of readdata_csv())\n",
    "    Output:\n",
    "        train_dataset[pandas.dataframe]: dataframe of the train dataset\n",
    "        test_dataset[pandas.dataframe]: dataframe of the test dataset\n",
    "    \"\"\"\n",
    "    train_dataset = pd.DataFrame()\n",
    "    test_dataset = pd.DataFrame()\n",
    "    folds_uuids = get_folds_uuids(fold_dir)\n",
    "    \n",
    "    # Dividing the folds uuids into train and test (the L denotes they are still lists)\n",
    "    test_uuids_L = [folds_uuids[test_fold]]\n",
    "    del(folds_uuids[test_fold])\n",
    "    train_uuids_L = folds_uuids\n",
    "    \n",
    "    # Transforming the list of arrays of uuids into a single uuids np.array\n",
    "    test_uuids = np.vstack(test_uuids_L)\n",
    "    train_uuids = np.vstack(train_uuids_L)\n",
    "    \n",
    "    # Now collecting the test and train dataset using concatenating\n",
    "    for i in train_uuids:\n",
    "        train_dataset = pd.concat([train_dataset,grand_dataset[i[0]]], axis=0, \\\n",
    "                                  ignore_index=True)\n",
    "    \n",
    "    for j in test_uuids:\n",
    "        test_dataset = pd.concat([test_dataset,grand_dataset[j[0]]], axis=0, \\\n",
    "                                 ignore_index=True)\n",
    "        \n",
    "    return(train_dataset,test_dataset)\n",
    "\n",
    "def get_folds_uuids(fold_dir):\n",
    "    \"\"\"\n",
    "    The function gets the directory where the the folds text files are located\n",
    "    and returns a list of five np.arrays in each of them the uuids of the\n",
    "    corresponding fold are stored.\n",
    "    \n",
    "    Input:\n",
    "        fold_dir[string]: holds the directory in which folds are\n",
    "    \n",
    "    Output:\n",
    "        folds_uuids[list]: a list of numpy arrays. Each array holds the uuids\n",
    "                    in that fold. ex.\n",
    "                    folds_uuids = [('uuid1','uuid2',...,'uuid12'),\n",
    "                                   ('uuid13','uuid14',...,'uuid24'),\n",
    "                                   ...,\n",
    "                                   ('uuid49','uuid50',...,'uuid60')]\n",
    "    \"\"\"\n",
    "    num_folds = 5\n",
    "    # folds_uuids is gonna be a list of np.arrays. each array is a set of uuids\n",
    "    folds_uuids = [0,1,2,3,4]\n",
    "    # This loop reads all 5 test folds (iphone and android) and stores uuids\n",
    "    for i in range(0,num_folds):\n",
    "        filename = 'fold_{}_test_android_uuids.txt'.format(i)\n",
    "        filepath = os.path.join(fold_dir, filename)\n",
    "        # aux1 is the uuids of ith test fold for \"android\"\n",
    "        aux1 = pd.read_csv(filepath,header=None,delimiter='\\n')\n",
    "        aux1 = aux1.values\n",
    "        \n",
    "        filename = 'fold_%s_test_iphone_uuids.txt' %i\n",
    "        filepath = os.path.join(fold_dir, filename)\n",
    "        # aux2 is the uuids of ith test fold for \"iphone\"\n",
    "        aux2 = pd.read_csv(filepath,header=None,delimiter='\\n')\n",
    "        aux2 = aux2.values\n",
    "        \n",
    "        # Then we concatenate them\n",
    "        folds_uuids[i] = np.concatenate((aux1,aux2),axis=0)\n",
    "        \n",
    "    return(folds_uuids)\n",
    "\n",
    "def train_NAPS_Models(train_dataset, feature_sets, j, Response_Perm_1, \\\n",
    "                      Response_Perm_2, i, bagging_R, num_bags, impute = True):\n",
    "    \n",
    "    t0 = timeit.default_timer()\n",
    "    X_train, y_train, y1, y2 = Xy(train_dataset, feature_sets, j, \\\n",
    "                        Response_Perm_1, Response_Perm_2, i, impute = True)\n",
    "    \n",
    "    t1 = timeit.default_timer()\n",
    "    # print('\\tgetting training data took : ', t1-t0)\n",
    "    \n",
    "    X_train_tmp, y_train_tmp = smt.fit_resample(X_train, y_train)\n",
    "    t2 = timeit.default_timer()\n",
    "    # print('\\tSMOTE took : ', t2-t1)\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train_tmp, columns = X_train.columns)\n",
    "    y_train = pd.DataFrame(y_train_tmp, columns = ['Merged_label'])\n",
    "    \n",
    "    \n",
    "    U2 = Uncertainty_Bias([y1, y2])\n",
    "    \n",
    "    t3 = timeit.default_timer()\n",
    "    #create a model and train it\n",
    "    NAPS_sample = DS_Model(Response_Perm_1[i], Response_Perm_2[i], \\\n",
    "               X_train, y_train, j)\n",
    "    t4 = timeit.default_timer()\n",
    "    # print('\\tCreating the DS model took : ', t4-t3)\n",
    "    \n",
    "    NAPS_sample.Bags_Trainer(X_train, y_train, bagging_R, num_bags)\n",
    "    t5 = timeit.default_timer()\n",
    "    # print('\\tTraining bags took : ', t5-t4)\n",
    "    \n",
    "    NAPS_sample.Uncertainty_B = U2\n",
    "    \n",
    "    return(NAPS_sample)\n",
    "\n",
    "#=============================================================================#\n",
    "#--------------------------| Tensorflow for LR |------------------------------#\n",
    "#=============================================================================#\n",
    " \n",
    "# num_classes = 10 # 0 to 9 digits\n",
    "# # num_features = 784 # 28*28\n",
    "# learning_rate = 0.01\n",
    "# training_steps = 1000\n",
    "# batch_size = 256\n",
    "# display_step = 50\n",
    "\n",
    "# W = tf.Variable(tf.ones([num_features, num_classes]), name=\"weight\")\n",
    "# b = tf.Variable(tf.zeros([num_classes]), name=\"bias\")\n",
    "\n",
    "# def logistic_regression(x):\n",
    "#     # Apply softmax to normalize the logits to a probability distribution.\n",
    "#     return tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# def cross_entropy(y_pred, y_true):\n",
    "#     # Encode label to a one hot vector.\n",
    "#     y_true = tf.one_hot(y_true, depth=num_classes)\n",
    "#     # Clip prediction values to avoid log(0) error.\n",
    "#     y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
    "#     # Compute cross-entropy.\n",
    "#     return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n",
    "\n",
    "# def accuracy(y_pred, y_true):\n",
    "#     # Predicted class is the index of the highest score in prediction vector (i.e. argmax).\n",
    "#     correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "#     return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# # Stochastic gradient descent optimizer.\n",
    "# optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    #=============================================================================#\n",
    "    #-------------------------------| INPUTS |------------------------------------#\n",
    "    #=============================================================================#\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    \n",
    "    #=============================================================================#\n",
    "    #-------------------------| Reading in the data |-----------------------------#\n",
    "    #=============================================================================#\n",
    "    \n",
    "    \n",
    "    Activities, Sensors = Set_Act_Sens() #creating two dicts for sensor and activity\n",
    "    \n",
    "    print('\\n#------------- Reading in the Data of Users -------------#\\n')\n",
    "    dataset_uuid = readdata_csv(data_dir) #reading all data and storing in \"dataset\" a DF\n",
    "    stop1 = timeit.default_timer()\n",
    "    \n",
    "    print('Reading the data took:   ', int(stop1 - start_time))\n",
    "    \n",
    "    uuids = list(dataset_uuid.keys())\n",
    "    \n",
    "    print('\\n#-------------- Combining the Data of Users-------------#\\n')\n",
    "    \n",
    "    #We concatenate the data of all participants (60) to get one dataset for all      \n",
    "    dataset_ag = dataset_uuid[uuids[0]] #\"dataset_ag\" is the aggregation of all user's data\n",
    "    for i in range(1,len(uuids)):\n",
    "        dataset_ag = pd.concat([dataset_ag, dataset_uuid[uuids[i]]], axis=0, ignore_index=True)\n",
    "    \n",
    "    dataset_ag.iloc[:,feature_range] = preprocessing.scale(dataset_ag.iloc[:,feature_range])\n",
    "    stop2 = timeit.default_timer()\n",
    "    \n",
    "    print('Combining the data took:   ', int(stop2 - stop1))\n",
    "    \n",
    "    #=============================================================================#\n",
    "    #-----------------------------| DST Setups |----------------------------------#\n",
    "    #=============================================================================#\n",
    "    \n",
    "    #We create feature sets, a sample mass function (initialized to 0) and response\n",
    "    #permutations 1 and 2 in which corresponding elements are exclusive and exhaustive\n",
    "    \n",
    "    feature_sets = feature_set(sensors_to_fuse, feature_sets_st, Sensors, feature_sets_count)\n",
    "    mass_template = BPA_builder(FOD)\n",
    "    Response_Perm_1, Response_Perm_2 = pair_resp(mass_template)\n",
    "    \n",
    "    num_p = len(FOD)\n",
    "    num_fs = len(feature_sets)\n",
    "    num_rp = len(Response_Perm_1)\n",
    "    num_folds = 5\n",
    "    \n",
    "    smt = SMOTE()\n",
    "    \n",
    "    #find the train_dataset\n",
    "    #at personal level:\n",
    "    print('\\n#-------------- Obtaining Training Dataset -------------#\\n')\n",
    "    \n",
    "    train_dataset, test_dataset = train_test_spl(0,num_folds,cvdir,dataset_uuid)\n",
    "    \n",
    "    stop3 = timeit.default_timer()\n",
    "    print('Obtaining the training dataset took:   ', int(stop3 - stop2))\n",
    "    \n",
    "    print('Training dataset has  {}  samples'.format(len(train_dataset)))\n",
    "    \n",
    "    \n",
    "    #=============================================================================#\n",
    "    #------------------| Creating and Training all the models |-------------------#\n",
    "    #=============================================================================#\n",
    "    \n",
    "    #------------------------ Parallelization goes here  -------------------------#\n",
    "    \n",
    "    #impute = True\n",
    "    #NAPS_Model = []\n",
    "    #with Pool(processes= num_prc) as pool:\n",
    "    #    pool.map(f, [1,2,3])\n",
    "    #\n",
    "    #if __name__ == '__main__':\n",
    "    #    # start 4 worker processes\n",
    "    #    with Pool(processes=4) as pool:\n",
    "    #\n",
    "    #        # print \"[0, 1, 4,..., 81]\"\n",
    "    #        print(pool.map(f, range(10)))\n",
    "    #    NAPS_Model += pool.map(NAPS_Models_Trainer, (num_rp, fs_range, train_dataset\\\n",
    "    #                                                  , feature_sets, Response_Perm_1\\\n",
    "    #                                                  , Response_Perm_2, impute))\n",
    "    \n",
    "    #NAPS_Models = NAPS_Models_Trainer(num_rp, fs_range, train_dataset, feature_sets\\\n",
    "    #                                  , Response_Perm_1, Response_Perm_2, impute = True)\n",
    "    \n",
    "    print('\\n#-------------- Creating and Training Models -------------#\\n')\n",
    "    \n",
    "    NAPS_models = []\n",
    "    print('\\nLooping over Response Permutations \\n ')\n",
    "    \n",
    "    for i in range(num_rp): #i runs over response permutations\n",
    "        \n",
    "        start_rp = timer()\n",
    "        \n",
    "        print('\\nResponse Permutation {}/{}'.format(i+1,num_rp))\n",
    "        print('\\n\\tLooping over feature sets')\n",
    "        NAPS_models.append([])\n",
    "        \n",
    "        progress = ProgressBar(num_fs, fmt = ProgressBar.FULL)\n",
    "        \n",
    "        for j in range(num_fs): #j runs over feature sets\n",
    "            NAPS_models[i].append([])\n",
    "            \n",
    "            NAPS_models[i][j] = \\\n",
    "                train_NAPS_Models(train_dataset, feature_sets, j, Response_Perm_1, \\\n",
    "                          Response_Perm_2, i, bagging_R, num_bags, impute = True)\n",
    "            #find X and y\n",
    "    #         X_train, y_train, y1, y2 = Xy(train_dataset, feature_sets, j, \\\n",
    "    #                         Response_Perm_1, Response_Perm_2, i, impute = True)\n",
    "    # #        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "            \n",
    "    #         X_train_tmp, y_train_tmp = smt.fit_resample(X_train, y_train)\n",
    "            \n",
    "    #         X_train = pd.DataFrame(X_train_tmp, columns = X_train.columns)\n",
    "    #         y_train = pd.DataFrame(y_train_tmp, columns = ['Merged_label'])\n",
    "            \n",
    "            \n",
    "    #         if j == 0:\n",
    "    #             U2 = Uncertainty_Bias([y1, y2])\n",
    "    \n",
    "    #         #create a model and train it\n",
    "    #         NAPS_models[i][j] = DS_Model(Response_Perm_1[i], Response_Perm_2[i], \\\n",
    "    #                    X_train, y_train, j)\n",
    "    #         NAPS_models[i][j].Bags_Trainer(X_train, y_train, bagging_R, num_bags)\n",
    "    #         NAPS_models[i][j].Uncertainty_B = U2\n",
    "            \n",
    "            progress.current += 1\n",
    "            progress()\n",
    "        \n",
    "        progress.done()\n",
    "        \n",
    "        stop_rp = timer()\n",
    "        \n",
    "        print(\"\\n It took : \", stop_rp - start_rp)\n",
    "    \n",
    "    stop4 = timeit.default_timer()\n",
    "    print('Training all models took:   ', int(stop4 - stop3))\n",
    "        \n",
    "    #=============================================================================#\n",
    "    #------------------| Model Selection and Testing Models |---------------------#\n",
    "    #=============================================================================#\n",
    "    \n",
    "    print('\\n#-------------- Testing the Models -------------#\\n')\n",
    "    test_dataset[FOD] = test_dataset[FOD].fillna(0)\n",
    "    test_dataset = test_dataset[np.sum(test_dataset[FOD], axis=1) != 0]\n",
    "    \n",
    "    y_test_ag = np.zeros([len(test_dataset),len(FOD)])\n",
    "    y_pred_ag = np.zeros([len(test_dataset),len(FOD)])\n",
    "    \n",
    "    \n",
    "    for t in range(len(test_dataset)):\n",
    "    # for t in range(50):\n",
    "        print(t,'/',len(test_dataset))\n",
    "        test_sample = test_dataset.iloc[t,:]\n",
    "        test_sample = test_sample.to_frame().transpose()\n",
    "        y_test_ag[t,:] = np.floor(test_sample[FOD].fillna(0).values)\n",
    "        \n",
    "        assert np.sum(y_test_ag == 1)\n",
    "        \n",
    "        Uncertainty_Mat = np.ones([num_rp, num_fs])\n",
    "        \n",
    "        for i in range(num_rp):\n",
    "            for j in range(num_fs):\n",
    "                X_test, y_test, y1, y2 = Xy(test_sample, feature_sets, j, \\\n",
    "                                        Response_Perm_1, Response_Perm_2, i, impute = True)\n",
    "                if len(X_test) != 0 or len(y_test) != 0:\n",
    "                    Uncertainty_Mat[i][j] = (NAPS_models[i][j].Uncertainty_B +\\\n",
    "                                    NAPS_models[i][j].Uncertainty_Context(X_test))/2\n",
    "                NAPS_models[i][j].Mass_Function_Setter(Uncertainty_Mat[i][j], X_test)\n",
    "                \n",
    "        #=========\\ Model Selection/==========#\n",
    "        \n",
    "        Selected_Models_idx = Model_Selector(Uncertainty_Mat, models_per_rp, num_rp, 1)\n",
    "        y_pred_ag[t,:] = Fuse_and_Predict(Selected_Models_idx, NAPS_models, FOD, num_p, \\\n",
    "                  num_rp, models_per_rp)\n",
    "    \n",
    "    stop5 = timeit.default_timer()\n",
    "    print('Testing took:   ', int(stop5 - stop4))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #conf_mat = confusion_matrix(y_test_ag, y_pred_ag)\n",
    "    accuracy = accuracy_score(y_test_ag, y_pred_ag)\n",
    "    #balanced_accuracy = balanced_accuracy_score(y_test_ag, y_pred_ag)\n",
    "    f1 = f1_score(y_test_ag, y_pred_ag, average='weighted')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #!!!!!!!!! Model Selection based on the uncertainty should be fixed !!!!!!!!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0de9a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time as tm\n",
    "import multiprocessing as mp\n",
    "def f(x):\n",
    "    return x**2-1\n",
    "\n",
    "f(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0451ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    start = tm.time()\n",
    "    with mp.Pool(processes=1) as pool:\n",
    "        print(pool.map(f, range(10)))\n",
    "\n",
    "    end = tm.time()\n",
    "    print(f'processing time is {end-start}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
